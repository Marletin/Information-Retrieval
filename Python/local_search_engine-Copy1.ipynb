{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML, display\n",
    "display(HTML('<style>.container { width:100%; !important } </style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "from PyPDF2 import PdfFileReader\n",
    "import filetype\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "import platform\n",
    "import operator\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, url, length, id, textList):\n",
    "        self.url = url\n",
    "        self.length = length\n",
    "        self.id = id\n",
    "        self.score = 0.\n",
    "        self.textList = textList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Index:\n",
    "    hashmap = {} #dictionary\n",
    "    fileCount = 0 #integer, Gesamtzahl aller gefunden Dateien\n",
    "    docHashmap = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildIndex(self):\n",
    "    \n",
    "    #startDirectories = self._getStartDirectories()\n",
    "    startDirectories = [\"F:/Jonas/IT/Springer-BÃ¼cher\"]\n",
    "    \n",
    "    for directory in startDirectories:\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                \n",
    "                path = os.path.abspath(os.path.join(root, file))\n",
    "                \n",
    "                try:\n",
    "                    if filetype.guess(path).mime == 'application/pdf':\n",
    "                        \n",
    "                        rawText = \"\"\n",
    "                        \n",
    "                        with open(path, 'rb') as f:\n",
    "                            pdf = PdfFileReader(f)\n",
    "                            numberOfPages = pdf.getNumPages()\n",
    "                            for number in range(numberOfPages):\n",
    "                                page = pdf.getPage(number)\n",
    "                                rawText += page.extractText()\n",
    "                        #fileData = parser.from_file(path)\n",
    "                        #rawText = fileData['content']\n",
    "                        self.fileCount += 1\n",
    "                    \n",
    "                        processedText = self._preprocessText(rawText)\n",
    "                        document = Document(path, len(processedText), self.fileCount, processedText)\n",
    "                        self.docHashmap.update({self.fileCount : document})\n",
    "                        self._addToIndex(self.fileCount, processedText)\n",
    "                except:\n",
    "                    #print(path)\n",
    "                    continue\n",
    "\n",
    "Index.buildIndex = buildIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _getStartDirectories(self):\n",
    "\n",
    "    if platform.system() == \"Linux\":\n",
    "        directories = [\"/\"]\n",
    "        \n",
    "    elif platform.system() == \"Darwin\":\n",
    "        directories = [\"/\"]\n",
    "        \n",
    "    elif platform.system() == \"Windows\":\n",
    "        directories = ['%s:\\\\' % d for d in string.ascii_uppercase if os.path.exists('%s:' % d)]\n",
    "        \n",
    "    else:\n",
    "        raise EnvironmentError\n",
    "        \n",
    "    return directories\n",
    "\n",
    "Index._getStartDirectories = _getStartDirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _addToIndex(self, documentID, terms):\n",
    "    \n",
    "    for term in terms:\n",
    "        \n",
    "        try:\n",
    "            docSet = self.hashmap[term]\n",
    "            docSet.add(documentID)\n",
    "            self.hashmap.update({term : docSet})\n",
    "            \n",
    "        except KeyError:\n",
    "            docSet = {documentID}\n",
    "            self.hashmap.update({term : docSet})\n",
    "    \n",
    "Index._addToIndex = _addToIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _preprocessText(self, text):\n",
    "    \n",
    "    lowerText = text.lower()\n",
    "    \n",
    "    prepText = re.sub(r'\\d+', '', lowerText)\n",
    "            \n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+-$|\\w+')\n",
    "    tokenList = tokenizer.tokenize(prepText)\n",
    "    \n",
    "    for token in tokenList:\n",
    "        if token[-1] == '-':\n",
    "            \n",
    "            index = tokenList.index(token)\n",
    "            compositeWord = token[:-1]+tokenList[index+1]\n",
    "            tokenList[index] = compositeWord\n",
    "            del tokenList[index+1]\n",
    "            \n",
    "    return tokenList\n",
    "    \n",
    "Index._preprocessText = _preprocessText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPROVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve(self, searchString):\n",
    " \n",
    "    processedStrings = self._preprocessText(searchString)\n",
    "    result, intsect = set(), set()\n",
    "    df, helpDict, helpDict2 = {}, {}, {}\n",
    "    resultList, resultList2 = [], []\n",
    "    \n",
    "    for word in processedStrings:\n",
    "        try:\n",
    "            documents = set(self.hashmap[word])\n",
    "            df[word] = len(documents)\n",
    "            intsect = result.intersection(documents).union(intsect)\n",
    "            result = result.union(documents)\n",
    "            \n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    for document in result:\n",
    "        doc = ind.docHashmap[document]\n",
    "        doc.tf_idf(processedStrings,df)\n",
    "        helpDict[doc.id] = doc.score\n",
    "        \n",
    "    for key in intsect:\n",
    "        helpDict2[key] = helpDict[key]\n",
    "        helpDict.pop(key)\n",
    "        \n",
    "    sortedDict = sorted(helpDict.items(), key=operator.itemgetter(1))\n",
    "    sortedDict2 = sorted(helpDict2.items(), key=operator.itemgetter(1))\n",
    "\n",
    "    \n",
    "    for key,_ in sortedDict:\n",
    "        resultList.append(ind.docHashmap[key].url)\n",
    "    \n",
    "    for key,_ in sortedDict2:\n",
    "        resultList2.append(ind.docHashmap[key].url)\n",
    "    \n",
    "    \n",
    "    resultList = resultList[::-1]\n",
    "    resultList2 = resultList2[::-1]\n",
    "    \n",
    "    return resultList2+resultList\n",
    "\n",
    "Index.retrieve = retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(self, termList, df):\n",
    "    \n",
    "    tfDict = {}\n",
    "    for term in termList:\n",
    "        tfDict[term] = 0\n",
    "    \n",
    "    ind = Index()\n",
    "        \n",
    "    for term in self.textList:\n",
    "        if term in termList:\n",
    "            tfDict[term] = tfDict[term]+1\n",
    "\n",
    "    for key, value in df.items():\n",
    "        idf = math.log((ind.fileCount/value+1),10)\n",
    "        tfDict[key] = tfDict[key]*idf\n",
    "    \n",
    "    self.score = sum(tfDict.values())\n",
    "\n",
    "Document.tf_idf = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind = Index()\n",
    "ind.buildIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultSet = ind.retrieve(\"sprache\")\n",
    "if resultSet:\n",
    "    for elem in resultSet:\n",
    "        if elem.split('.')[-1] != 'dat':\n",
    "            print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index in [1, 2, 166, 137, 138, 171, 210]:\n",
    "    print(ind.docHashmap[index].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(ind.docHashmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
