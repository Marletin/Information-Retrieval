{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beispiel-Implementierung: Lokale Suchmaschine\n",
    "\n",
    "## Ziel der Beispiel-Implementierung\n",
    "Im Folgenden wird eine Anwendung der zuvor theoretisch diskutierten Inhalte vorgestellt. Dabei soll eine lokale Suchmaschine entwickelt werden, welche in der Lage ist, pdf-Dateien auf einem lokalen Computer-System zu parsen, in einen invertierten Index aufzunehmen sowie Suchanfragen eines Benutzers sinnvoll zu beantworten. <br>\n",
    "Zur Relevanz-Bestimmung der Dokumente wird das TF-IDF-Maß, welches bereits vorgestellt wurde, genutzt. Um den Index zu speichern, wird die von Python mitgelieferte Datenstruktur \"dictionary\", welche im Grunde eine Hashmap ist, genutzt.\n",
    "Weiter werden einige Bibliotheken eingesetzt, welche einige Vorarbeit leisten und damit den Code der Beispiel-Implementierung auf das Wesentliche beschränken. So soll die grundlegende Arbeitsweise eines Information Retrieval-Systems dargelegt werden.\n",
    "\n",
    "## Genutzte Bibliotheken\n",
    "Bevor mit der eigentlichen Implementierung der lokalen Suchmaschine begonnen werden kann, müssen einige Bibliotheken eingebunden werden. Darunter fallen Apache Tika, das Math-Modul von Python, os (um auf die Directories zugreifen zu können), python-magic, regular expressions (re) (und noch weitere, bei Bedarf einfügen!). <br>\n",
    "\n",
    "### Tika\n",
    "Tika liefert eine Parser, mit dessen Hilfe der Text aus - unter anderem - pdf-Dateien extrahiert werden kann.\n",
    "Mit dem Aufruf _parser.from_\\__file(file)_ kann eine pdf-Datei in reinen Text umgewandelt werden. Die Funktion liefert ein Dictionary zurück, welches einen Key _content_ besitzt, über den auf den Inhalt der pdf-Datei zugegriffen werden kann.\n",
    "\n",
    "### python-magic\n",
    "Mittels python-magic ist es möglich, unabhängig von der Dateiendung, den Typ einer Datei zu ermitteln. Dies hat den Vorteil, dass die Suchmaschine sowohl unter Windows, als auch unter Unix-Systemen, alle pdf-Dateien finden kann, da unter Unix die Dateiendung keine garantierten Rückschlüsse auf den Typ der Datei zulässt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import magic\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Document-Klasse\n",
    "Das Speichern er für das Retrieval wichtigen Informationen, geschieht mittels einer Document-Klasse. Diese Klasse hält alle Attribute, die wichtig sind, um das TF-IDF-Maß berechnen zu können. Diese Attribute sind:\n",
    "- url\n",
    "- length\n",
    "- id\n",
    "\n",
    "Die Variable _url_ ist ein String und enthält den Pfad zum Dokument, welches durch das entsprechende Document-Objekt repräsentiert wird. _length_ ist ein Integer und beinhaltet die Anzahl der Wörter, die in dem Dokument vorkommen und _id_ ist die eindeutige Dokumenten-ID, zu der weiter unten noch genaueres gesagt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, url, length, id):\n",
    "        self.url = url\n",
    "        self.length = length\n",
    "        self.id = id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Der Index\n",
    "Nachdem die benötigten Bibliotheken bekannt sind, kann der Index implementiert werden. Bevor dieser jedoch aufgebaut werden kann, sind einige Vorarbeiten nötig, die durch die vorgestellten Bibliotheken gestützt werden.\n",
    "Der Index wird im Folgenden als Klasse implementiert. Diese beinhaltet die folgenden Methoden, die in den folgenden Abschnitten genauer diskutiert werden:\n",
    "- buildIndex()\n",
    "- retrieve()\n",
    "- calcTFIDF()\n",
    "\n",
    "Weiter werden die folgenden Member-Variablen benötigt:\n",
    "- hashmap\n",
    "- fileCount\n",
    "- docHashmap\n",
    "\n",
    "Die Member-Variable _hashmap_ ordnet allen Termen eine Liste von eindeutigen Dokumenten-IDs zu, in denen sie vorkommen. Im Dictionary _docHashmap_ werden die Dokuemnten-IDs als Key genutzt, um eine Zurodnung von Dokumenten-IDs auf Document-Objekte zu ermöglichen. Die Variable _fileCount_ ist ein Integer und wird für jedes gefundene Dokument um _1_ hochgezählt. Damit ist diese Variable qualifiziert als eindeutige Dokumenten-ID zu fungieren, wofür sie genutzt wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Index:\n",
    "    hashmap = {} #dictionary\n",
    "    fileCount = 0 #integer, Gesamtzahl aller gefunden Dateien\n",
    "    docHashmap = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## buildIndex\n",
    "Die Methode _buildIndex_ baut - wie der Name bereits vermuten lässt - den Index auf. Dabei dient ein Dictionary als Basis-Datenstruktur. Zudem führt diese Methode die Verarbeitung der pdf-Dateien mittels Apache tika und python-magic durch.\n",
    "\n",
    "Der erste Schritt stellt das Iterieren über alle Directories dar. Gestartet wird bei Linux-Systemen im Root-Directory, unter Windows-Systemen muss über jede Partition iteriert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildIndex(self):\n",
    "    # alle Start-Verzeichnisse holen\n",
    "    #start = self.__getStartDirectories()\n",
    "    start = [\"F:/Jonas/Uni/Arbeiten/Information Retrieval/Information-Retrieval/Latex\"]\n",
    "    # Magic-Instanz erstellen, um Datei-Typ bestimmen zu können\n",
    "    mime = magic.Magic(mime=True)\n",
    "    \n",
    "    for s in start:\n",
    "        for subdir, dir, files in os.walk(s):\n",
    "            for f in files:\n",
    "                path = s+\"/\"+f\n",
    "                if mime.from_file(path) == \"application/pdf\":\n",
    "                    # in Text umwawndeln und tokenization durchführen\n",
    "                    fileData = parser.from_file(s+\"/\"+f)\n",
    "                    rawText = fileData['content']\n",
    "                    self.fileCount += 1\n",
    "                    \n",
    "                    processedText = self.__preprocessText(rawText)\n",
    "                    document = Document(path, len(processedText), self.fileCount)\n",
    "                    self.docHashmap.update({self.fileCount : document})\n",
    "                    self.__addToIndex(self.fileCount, processedText)\n",
    "                    \n",
    "    return\n",
    "\n",
    "Index.buildIndex = buildIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsmethoden\n",
    "In diesem Abschnitt werden die genutzten Hilfsmethoden kurz vorgestellt. Diese werden jedoch nicht in der Tiefe behandelt, wie die drei Haupt-Methoden behandelt werden. \n",
    "\n",
    "#### __getStartDirectories\n",
    "Die Methode _\\_\\_getSartDirectories_ liefert eine Liste zurück, welche abhängig vom Betriebssystem, auf dem die Suchmaschine läuft, die Start-Verzeichnisse zurückgibt, in denen nach pdf-Dateien gesucht werden soll. Falls das zugrunde liegende Betriebssystem ein Linux-basiertes System ist, wird die Liste __[\"/\"]__ zurückgegeben, falls ein Windows-System zugrundeliegt, wird die Liste aller Partitionen zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __getStartDirectories(self):\n",
    "    start = []\n",
    "    \n",
    "    if platform.system() == \"Linux\":\n",
    "        start.append(\"/\")\n",
    "    elif platform.system() == \"Windows\":\n",
    "        start = ['%s:' % d for d in string.ascii_uppercase if os.path.exists('%s:' % d)]\n",
    "    else:\n",
    "        raise EnvironmentError\n",
    "        \n",
    "    return start\n",
    "\n",
    "Index.__getStartDirectories = __getStartDirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __addToIndex\n",
    "\n",
    "Diese Methode bekommt als Argument einen String, welcher den von tika extrahierten Text enthält. Dieser String wird mithilfe der in Python enthaltenen Methode _split_ aufgeteilt und in einer Liste zusammengefasst. Weiter werden Satzzeichen wie Punkte, Kommata, etc. aus dem String bzw. der Liste herausgefiltert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __addToIndex(self, document, terms):\n",
    "    for t in terms:\n",
    "        try:\n",
    "            docs = self.hashmap[t]\n",
    "            if document not in docs:\n",
    "                docs.append(document)\n",
    "                self.hashmap.update({t : docs})\n",
    "        except KeyError:\n",
    "            docs = [document]\n",
    "            self.hashmap.update({t : docs})\n",
    "    \n",
    "Index.__addToIndex = __addToIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __preprocessText\n",
    "Diese Methode dient der Vorverarbeitung der Texte, die in den pdf-Dokumenten stehen. Als erster Schritt wird der gesamte Text in Lower-Case gesetzt, damit bei der Suche später die Groß- bzw. Kleinschreibung unwichtig ist.\n",
    "Im nächsten Schritt werden alle Zahlen aus dem Text entfernt.\n",
    "Innerhalb der for-Schleife werden Trennungen von Wörtern, die auf zwei Zeilen aufgeteilt wurden, wieder zusammengefügt. Als letzter Schritt werden mithilfe des von _nltk_ migelieferten _RegexpTokenizer_ Tokens gebildet. Diese Tokens entsprechen hier beriets den Termen, die dann im Dictionary des Index aufgenommen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __preprocessText(self, txt):\n",
    "    # lower all:\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    # remove digits\n",
    "    txt = re.sub(r'\\d+', '', txt)\n",
    "            \n",
    "    # tokenize the text\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+-$|\\w+')\n",
    "    result = tokenizer.tokenize(txt)\n",
    "    \n",
    "    # concatenate divided words\n",
    "    for word in result:\n",
    "        if word[-1] == '-':\n",
    "            ind = result.index(word)\n",
    "            corrected = word[:-1]+result[ind+1]\n",
    "            result[ind] = corrected\n",
    "            del result[ind+1]\n",
    "    return result\n",
    "    \n",
    "Index.__preprocessText = __preprocessText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### retrieve\n",
    "Die retrieve-Methode dient der Suche. Die Idee dabei ist, dass der Nutzer ein oder mehrere Schlagworte eingeben kann, auf deren Basis die am besten passenden Dokumente zurückgeliefert werden. Auch die Schlagworte werden den gleichen Normalisierungs-Prozess durchlaufen wie die Texte der Dokumente.\n",
    "\n",
    "Diese Methode ist - im Gegensatz zu den vorherigen Methoden - mengenbasiert. Dies hat folgenden Hintergrund: Wenn der Nutzer mehrere Schlagworte eingibt, ist es möglich, dass mehrere Schlagworte in den gleichen Dokumenten vorkommen. Damit die Dokumente nicht doppelt in der Ergebnis-Liste vorkommen, werden Mengen verwendet, da in Mengen per Definition jedes Element nur ein Mal enthalten sein darf.\n",
    "\n",
    "Die Implementierung dieser Methode ist hier zu sehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve(self, searchString):\n",
    "    # pre-processing\n",
    "    processedStrings = self.__preprocessText(searchString)\n",
    "    result = set()\n",
    "    for word in processedStrings:\n",
    "        documents = set(self.hashmap[word])\n",
    "        result = result.union(documents)\n",
    "        \n",
    "    return result\n",
    "\n",
    "Index.retrieve = retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind = Index()\n",
    "ind.buildIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1}\n",
      "F:/Jonas/Uni/Arbeiten/Information Retrieval/Information-Retrieval/Latex/main.pdf\n"
     ]
    }
   ],
   "source": [
    "print(ind.retrieve(\"Information\"))\n",
    "print(ind.docHashmap[1].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
