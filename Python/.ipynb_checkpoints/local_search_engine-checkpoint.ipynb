{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100%; !important } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML, display\n",
    "display(HTML('<style>.container { width:100%; !important } </style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beispiel-Implementierung: Lokale Suchmaschine\n",
    "\n",
    "## Ziel der Beispiel-Implementierung\n",
    "Im Folgenden wird eine Anwendung der zuvor theoretisch diskutierten Inhalte vorgestellt. Dabei soll eine lokale Suchmaschine entwickelt werden, welche in der Lage ist, pdf-Dateien auf einem lokalen Computer-System zu parsen, in einen invertierten Index aufzunehmen sowie Suchanfragen eines Benutzers sinnvoll zu beantworten. <br>\n",
    "Zur Relevanz-Bestimmung der Dokumente wird das TF-IDF-Maß, welches bereits vorgestellt wurde, genutzt. Um den Index zu speichern, wird die von Python mitgelieferte Datenstruktur \"dictionary\", welche im Grunde eine Hashmap ist, genutzt.\n",
    "Weiter werden einige Bibliotheken eingesetzt, welche einige Vorarbeit leisten und damit den Code der Beispiel-Implementierung auf das Wesentliche beschränken. So soll die grundlegende Arbeitsweise eines Information Retrieval-Systems dargelegt werden.\n",
    "\n",
    "## Genutzte Bibliotheken\n",
    "Bevor mit der eigentlichen Implementierung der lokalen Suchmaschine begonnen werden kann, müssen einige Bibliotheken eingebunden werden. Darunter fallen Apache Tika, das Math-Modul von Python, os (um auf die Directories zugreifen zu können), python-magic, regular expressions (re) (und noch weitere, bei Bedarf einfügen!). <br>\n",
    "\n",
    "### Tika\n",
    "Tika liefert eine Parser, mit dessen Hilfe der Text aus - unter anderem - pdf-Dateien extrahiert werden kann.\n",
    "Mit dem Aufruf _parser.from_\\__file(file)_ kann eine pdf-Datei in reinen Text umgewandelt werden. Die Funktion liefert ein Dictionary zurück, welches einen Key _content_ besitzt, über den auf den Inhalt der pdf-Datei zugegriffen werden kann.\n",
    "\n",
    "### python-magic\n",
    "Mittels python-magic ist es möglich, unabhängig von der Dateiendung, den Typ einer Datei zu ermitteln. Dies hat den Vorteil, dass die Suchmaschine sowohl unter Windows, als auch unter Unix-Systemen, alle pdf-Dateien finden kann, da unter Unix die Dateiendung keine garantierten Rückschlüsse auf den Typ der Datei zulässt.\n",
    "\n",
    "### nltk\n",
    "Die Bibliothek nltk (natural language toolkit) wird verwendet, um die Eingabetexte der Dokumente und die Eingaben des Nutzers zu normalisieren. Zudem wird Stemming mithilfe von nltk durchgeführt, um Wörter auf ihren Wortstamm zurückzuführen. In den unteren Methoden wird näheres über die genutzten Operationen erläutert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import magic\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "import platform\n",
    "import re\n",
    "import operator\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Document-Klasse\n",
    "Das Speichern er für das Retrieval wichtigen Informationen, geschieht mittels einer Document-Klasse. Diese Klasse hält alle Attribute, die wichtig sind, um das TF-IDF-Maß berechnen zu können. Diese Attribute sind:\n",
    "- url\n",
    "- length\n",
    "- id\n",
    "\n",
    "Die Variable _url_ ist ein String und enthält den Pfad zum Dokument, welches durch das entsprechende Document-Objekt repräsentiert wird. _length_ ist ein Integer und beinhaltet die Anzahl der Wörter, die in dem Dokument vorkommen und _id_ ist die eindeutige Dokumenten-ID, zu der weiter unten noch genaueres gesagt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, url, length, id, textList):\n",
    "        self.url = url\n",
    "        self.length = length\n",
    "        self.id = id\n",
    "        self.score = 0.\n",
    "        self.textList = textList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Der Index\n",
    "Nachdem die benötigten Bibliotheken bekannt sind, kann der Index implementiert werden. Bevor dieser jedoch aufgebaut werden kann, sind einige Vorarbeiten nötig, die durch die vorgestellten Bibliotheken gestützt werden.\n",
    "Der Index wird im Folgenden als Klasse implementiert. Diese beinhaltet die folgenden Methoden, die in den folgenden Abschnitten genauer diskutiert werden:\n",
    "- buildIndex()\n",
    "- retrieve()\n",
    "- calcTFIDF()\n",
    "\n",
    "Weiter werden die folgenden Member-Variablen benötigt:\n",
    "- hashmap\n",
    "- fileCount\n",
    "- docHashmap\n",
    "\n",
    "Die Member-Variable _hashmap_ ordnet allen Termen eine Menge von eindeutigen Dokumenten-IDs zu, in denen sie vorkommen. Im Dictionary _docHashmap_ werden die Dokuemnten-IDs als Key genutzt, um eine Zurodnung von Dokumenten-IDs auf Document-Objekte zu ermöglichen. Die Variable _fileCount_ ist ein Integer und wird für jedes gefundene Dokument um _1_ hochgezählt. Damit ist diese Variable qualifiziert als eindeutige Dokumenten-ID zu fungieren, wofür sie genutzt wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    hashmap = {} #dictionary\n",
    "    fileCount = 0 #integer, Gesamtzahl aller gefunden Dateien\n",
    "    docHashmap = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## buildIndex\n",
    "Die Methode _buildIndex_ baut - wie der Name bereits vermuten lässt - den Index auf. Dabei dient ein Dictionary als Basis-Datenstruktur.\n",
    "\n",
    "Der erste Schritt stellt das Iterieren über alle Directories dar. Gestartet wird bei Linux-Systemen im Root-Directory, unter Windows-Systemen muss über jede Partition iteriert werden. Als nächstes wird über alle Dateien in den Verzeichnissen iteriert. Für jede Datei wird durch python-magic ermittelt, ob es sich um ein pdf-Dokument handelt. Ist ein Dokument vom Typ _pdf_, wird mithilfe von tika der Text aus dem pdf-Dokument extrahiert. \n",
    "\n",
    "Für jede entdeckte pdf-Datei wird ein Zähler erhöht, welcher eine eindeutige Dokumenten-ID darstellt. Anschlißend wird mittels der Hilfsmethode _\\__processText_ der Text der pdf-Dateien normalisiert. Diese Methode wird weiter unten genauer betrachtet.\n",
    "\n",
    "Die letzten Schritte beinhalten das Anlegen eines neuen Dokumenten-Objekts, welches in das Dictionary _docHashmap_ eingefügt wird. Zudem wird die Dokumenten-ID mithilfe der Hilfsmethode  _\\__addToIndex_ dem Dictionary _hasmap_ hinzugefügt, welches den eigentlichen Index enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildIndex(self):\n",
    "    \n",
    "    startDirectories = self._getStartDirectories()\n",
    "    #startDirectories = [Userdefined path]\n",
    "    mime = magic.Magic(mime=True)\n",
    "    \n",
    "    for directory in startDirectories:\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                \n",
    "                path = os.path.abspath(os.path.join(root, file))\n",
    "                \n",
    "                try:\n",
    "                    if mime.from_file(path) == \"application/pdf\":\n",
    "\n",
    "                        fileData = parser.from_file(path)\n",
    "                        rawText = fileData['content']\n",
    "                        self.fileCount += 1\n",
    "                    \n",
    "                        processedText = self._preprocessText(rawText)\n",
    "                        document = Document(path, len(processedText), self.fileCount, processedText)\n",
    "                        self.docHashmap.update({self.fileCount : document})\n",
    "                        self._addToIndex(self.fileCount, processedText)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "    return\n",
    "\n",
    "Index.buildIndex = buildIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsmethoden\n",
    "In diesem Abschnitt werden die genutzten Hilfsmethoden kurz vorgestellt. Diese werden jedoch nicht in der Tiefe behandelt, wie die drei Haupt-Methoden behandelt werden. \n",
    "\n",
    "#### _getStartDirectories\n",
    "Die Methode _\\__getSartDirectories_ liefert eine Liste zurück, welche abhängig vom Betriebssystem, auf dem die Suchmaschine läuft, die Start-Verzeichnisse zurückgibt, in denen nach pdf-Dateien gesucht werden soll. Falls das zugrunde liegende Betriebssystem ein Linux-basiertes System ist, wird die Liste __[\"/\"]__ zurückgegeben, falls ein Windows-System zugrundeliegt, wird die Liste aller Partitionen zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getStartDirectories(self):\n",
    "    start = []\n",
    "    \n",
    "    if platform.system() == \"Linux\":\n",
    "        start.append(\"/\")\n",
    "    elif platform.system() == \"Windows\":\n",
    "        start = ['%s:\\\\' % d for d in string.ascii_uppercase if os.path.exists('%s:' % d)]\n",
    "    else:\n",
    "        raise EnvironmentError\n",
    "        \n",
    "    return start\n",
    "\n",
    "Index._getStartDirectories = _getStartDirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _addToIndex\n",
    "\n",
    "Diese Methode bekommt als Argumente eine Liste von Termen, die in einem pdf-Dokument vorkommen. Zudem wird eine eindeutige Dokumenten-ID übergeben.\n",
    "\n",
    "Für jeden Term in _terms_ wird versucht, eine Menge mit Dokumenten-IDs aus dem Index zu holen. Existiert bereits eine Menge, wird kein Fehler geworfen.\n",
    "\n",
    "Schlägt der Versuch, die Menge für den Term _term_ aus dem Dictionary zu holen, fehl, existiert noch keine Menge. In diesem Fall wird eine neue Menge erstellt und für den Term _term_ ein Eintrag im Dictionary hinzugefügt, der auf die neu erstellte Menge referenziert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _addToIndex(self, documentID, terms):\n",
    "    for term in terms:\n",
    "        try:\n",
    "            docs = self.hashmap[term]\n",
    "            docs.add(documentID)\n",
    "            self.hashmap.update({term : docs})\n",
    "        except KeyError:\n",
    "            docs = {documentID}\n",
    "            self.hashmap.update({term : docs})\n",
    "    \n",
    "Index._addToIndex = _addToIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _preprocessText\n",
    "Diese Methode dient der Vorverarbeitung der Texte, die in den pdf-Dokumenten stehen. Hierfür wird der Text eines PDF-Dokumentes an den Parametet _text_ übergeben. Als erster Schritt wird der gesamte Text in Lower-Case (Kleinschreibung) gesetzt, damit später bei der Suche die Groß- bzw. Kleinschreibung irrelevant ist. Das Ergebnis wird in der Variable _lowerText_ gespeichert.\n",
    "Im nächsten Schritt werden alle Zahlen aus _lowerText_ entfernt und in _prepText_ gespeichert, da Zahlen für die Textsuche nicht von Bedeutung sind.\n",
    "\n",
    "Als nächstes wird mithilfe der Klasse _RegexpTokenizer_, die durch die nltk-Bibliothek zur Verfügung gestellt wird, der String _prepText_ in eine Liste von Tokens aufgespalten. Was als Token gewertet wird, wird mithilfe einer _regular expression_ definiert, im Deutschen regulärer Ausdruck genannt. Dieser reguläre Ausdruck wird dem Konstruktor der _RegexpTokenizer_-Klasse in Form eines raw Strings übergeben. Ein raw String ist ein String welcher mit einem _r_ am Anfang gekennzeichnet ist und ein Backslash (\\\\) als ein Literal behandelt und nicht als Escape-Zeichen. Dies ist bei regulären Ausdrücken nützlich, da in diesen viel mit Backslashes gearbeitet wird.\n",
    "\n",
    "Im Folgenden wird der raw String näher betrachtet, um zu verstehen, was als ein Token gewertet wird. Der erste Teil des regulären Ausdrucks _[a-zA-Z]+-\\$_ definiert alle Buchstabenketten mit einen oder mehreren Elementen, die mit einem Miuszeichen bzw. Bindestrich enden, als Token. Die eckigen Klammern werden bei regulären Ausdrücken genutzt um eine Zeichenauswahl zu definieren. Das bedeutet das ein Zeichen aus dieser Auswahl Das Pluszeichen hinter diesen Klammern sagt aus, das ein oder mehr als ein Zeichen dieser Gruppe erwartet wird. Das Dollarzeichen definiert das Ende eines Wortes, in unserem Fall Tokens, und bezieht sich auf das vorangehende Zeichen, also den Bindestrich. So bedeutet _-$_, dass das Wort auf einem Bindestrich endet. Das Oderzeichen (|) verknüpft diesen regulären Ausdruck mit dem zweiten.\n",
    "Der zweite reguläre Ausdruck _\\w+_ definiert alle alphanumerischen Zeichenketten mit einem oder mehreren Elementen als Token. \n",
    "Mithilfe der _tokenize_-Methode wird die _regular expression_ auf den übergebenen String angewendet. Jeder Substring des mitgegebenen Arguments, der die _regular expression_ erfüllt, wird einer Liste angefügt.\n",
    "\n",
    "Der Bindestrich ist wichtig, da mit dessen Hilfe, alle Wörter gefunden werden können, die im Text aufgrund eines Zeilenumbruchs getrennt wurden. In der for-SChleife werden die Wörter auf die Eigenschaft, mit einem Bindestrich zu enden, geprüft und gegebenenfalls zusammengesetzt. Dazu wird der Bindestrich aus dem Wort entfernt (_word[-1]_) und mit dem nächsten word in der Tokenliste verknüpft\n",
    "\n",
    "Diese Methode ist jedoch nicht immer korrekt, denn es kann auch folgender Fall eintreten: Eine Zeile endet mit z.B. Damen-, die nächste Zeile geht mit z.B. und Herrenschuhe weiter. In diesem Fall ist der Bindestrich gewollt, der Algorithmus fügt jedoch die Wörter _Damen_ und _und_ zu einem Wort zusammen. Da dieser Fall jedoch sehr selten auftritt, wird in Kauf genommen, dass ab und zu die Wörter falsch zusammengefügt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocessText(self, text):\n",
    "    \n",
    "    lowerText = text.lower()\n",
    "    \n",
    "    prepText = re.sub(r'\\d+', '', lowerText)\n",
    "            \n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+-$|\\w+')\n",
    "    tokenList = tokenizer.tokenize(prepText)\n",
    "    \n",
    "    for token in tokenList:\n",
    "        if token[-1] == '-':\n",
    "            \n",
    "            index = tokenList.index(token)\n",
    "            compositeWord = token[:-1]+tokenList[index+1]\n",
    "            tokenList[index] = compositeWord\n",
    "            del tokenList[index+1]\n",
    "            \n",
    "    return result\n",
    "    \n",
    "Index._preprocessText = _preprocessText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieve\n",
    "Die retrieve-Methode dient der Suche. Die Idee dabei ist, dass der Nutzer ein oder mehrere Schlagworte eingeben kann, auf deren Basis die am besten passenden Dokumente zurückgeliefert werden. Auch die Schlagworte werden den gleichen Normalisierungs-Prozess durchlaufen wie die Texte der Dokumente.\n",
    "\n",
    "Zunächst wird der Such-String des Nutzers mittels der bereits bekannten Methode _\\__preprocessText_ normalisiert. Im zweiten Schritt wird eine leere Menge angelegt, in der die Dokumenten-IDs, die zu den Termen gefunden werden, gespeichert. Mithilfe der for-Schleife wird über _processedStrings_ iteriert. Für jedes Wort wird versucht, die Menge aller Dokumenten-IDs zu dem Term _word_ aus dem Index zu beschaffen. Existiert die Menge zu dem Term _word_, wird die Vereinigung der bereits in der Menge _result_ stehenden Dokumenten-IDs und der mit dem Term _word_ gefundenen Dokumenten-IDs gebildet. Existiert der Term _word_ nicht als Key im Index, wird beim nächsten Term der Liste _processedStrings_ fortgefahren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(self, searchString):\n",
    "    # pre-processing\n",
    "    processedStrings = self._preprocessText(searchString)\n",
    "    result = set()\n",
    "    df = {}\n",
    "    helpDict = {}\n",
    "    resultList = []\n",
    "    \n",
    "    for word in processedStrings:\n",
    "        try:\n",
    "            documents = set(self.hashmap[word])\n",
    "            df[word] = len(documents)\n",
    "            result = result.union(documents)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    for document in result:\n",
    "        doc = ind.docHashmap[document]\n",
    "        doc.tf_idf(processedStrings,df)\n",
    "        helpDict[doc.id] = doc.score\n",
    "        \n",
    "    sortedDict = sorted(helpDict.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    for key,_ in sortedDict:\n",
    "        resultList.append(ind.docHashmap[key].url)\n",
    "        \n",
    "    return resultList[::-1]\n",
    "\n",
    "Index.retrieve = retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-29 10:31:36,958 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
      "2019-03-29 11:41:13,520 [MainThread  ] [WARNI]  Tika server returned status: 422\n",
      "2019-03-29 11:41:13,656 [MainThread  ] [WARNI]  Tika server returned status: 422\n",
      "2019-03-29 11:41:13,916 [MainThread  ] [WARNI]  Tika server returned status: 422\n"
     ]
    }
   ],
   "source": [
    "ind = Index()\n",
    "ind.buildIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Die tf_idf-Methode wird in die Dokumentenklasse implementiert Für die Berechnung des TF-IDF-Maßes werden folgende Werte benötigt:\n",
    "\n",
    "- die Terme, welche gesucht werden bzw. in die Query eingegeben wurden\n",
    "- für jeden Term die Anzahl der Vorkommnisse im Dokument\n",
    "- für jeden Term die Anzahl der Dokumente, welche für den einen Term gefunden wurden (Document Frequency)\n",
    "- die Anzahl der Dokumente in der Kollektion\n",
    "- d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(self, termList, df):\n",
    "    tfDict = {}\n",
    "    for term in termList:\n",
    "        tfDict[term] = 0\n",
    "    \n",
    "    ind = Index()\n",
    "        \n",
    "    for term in self.textList:\n",
    "        if term in termList:\n",
    "            tfDict[term] = tfDict[term]+1\n",
    "\n",
    "    for key, value in df.items():\n",
    "        idf = math.log((ind.fileCount+1/value+1),10)\n",
    "        tfDict[key] = tfDict[key]*idf\n",
    "    \n",
    "    self.score = sum(tfDict.values())\n",
    "\n",
    "Document.tf_idf = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\MySQL\\MySQL Documentation 5.7\\refman-5.7-en.a4.pdf\n",
      "C:\\Program Files (x86)\\MySQL\\MySQL Documentation 5.7\\refman-5.7-en.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Wissenbasierte Systeme\\artificial-intelligence.pdf\n",
      "C:\\Users\\marle\\AppData\\Local\\Programs\\MiKTeX 2.9\\doc\\latex\\listings\\lstdrvrs.pdf\n",
      "C:\\Program Files\\Setlx\\tutorial.pdf\n",
      "C:\\Users\\marle\\AppData\\Local\\Programs\\MiKTeX 2.9\\doc\\asymptote\\asymptote.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Algorithmen\\algorithms.pdf\n",
      "C:\\Uni\\Studium\\Logik\\algorithms.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Logik\\algorithms.pdf\n",
      "C:\\Uni\\Studium\\Praxisbericht_2\\Monitoring-LAPTOP-2NADN4MG-2.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Praxisbericht_2\\Monitoring-LAPTOP-2NADN4MG-2.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Wissensmanagement\\artificial-intelligence.pdf\n",
      "C:\\Uni\\Studium\\Wissensmanagement\\artificial-intelligence.pdf\n",
      "C:\\Github\\Information-Retrieval\\Latex\\main.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Praxisbericht_2\\Monitoring.pdf\n",
      "C:\\Uni\\Studium\\Statistik\\statistik.pdf\n",
      "C:\\Uni\\Studium\\Analysis\\analysis.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Statistik\\statistik.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Analysis\\analysis.pdf\n",
      "C:\\Uni\\Studium\\Praxisbericht_2\\Monitoring-LAPTOP-2NADN4MG.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Praxisbericht_2\\Monitoring-LAPTOP-2NADN4MG.pdf\n",
      "C:\\Users\\marle\\AppData\\Local\\Programs\\MiKTeX 2.9\\doc\\latex\\listings\\listings.pdf\n",
      "C:\\Uni\\Studium\\Logik\\logic.pdf\n",
      "C:\\Uni\\Studium\\Lineare Algebra\\lineare-algebra.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Logik\\logic.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Lineare Algebra\\lineare-algebra.pdf\n",
      "C:\\Users\\marle\\Downloads\\main.pdf\n",
      "C:\\Users\\marle\\Downloads\\main (1).pdf\n",
      "C:\\Windows\\System32\\DriverStore\\FileRepository\\nvaci.inf_amd64_fa38828e445984f5\\nvidia-smi.1.pdf\n",
      "C:\\Uni\\Studium\\Studienarbeit\\irbookonlinereading.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Studienarbeit\\irbookonlinereading.pdf\n",
      "C:\\Github\\Information-Retrieval\\Sources\\irbookonlinereading.pdf\n",
      "C:\\Program Files\\Setlx\\manual.pdf\n",
      "C:\\Users\\marle\\.gradle\\wrapper\\dists\\gradle-4.1-all\\bzyivzo6n839fup2jbap0tjew\\gradle-4.1\\docs\\userguide\\userguide.pdf\n",
      "C:\\Users\\marle\\AppData\\Local\\Programs\\MiKTeX 2.9\\doc\\generic\\hyph-utf8\\es\\division.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Datenbanken II\\2018_Book_SQLServer2017QueryPerformanceT.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Web Engineering 2\\1.2-HTTP.pdf\n",
      "C:\\Users\\marle\\AppData\\Local\\Programs\\MiKTeX 2.9\\doc\\dvipdfmx\\dvipdfmx-special.pdf\n",
      "C:\\Privat\\Python-Hacking-Foliensatz.pdf\n",
      "C:\\Uni\\Studium\\Java\\Klausurrelevant.pdf\n",
      "C:\\Uni\\Studium\\Java\\Folien.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Big Data\\Vorlesung\\BigDataAnalytics-Lecture-05.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Big Data\\Vorlesung\\BigDataAnalytics-Lecture-04.pdf\n",
      "C:\\Uni\\Studium\\Formale Sprachen & Automaten\\KIT_Formale-Sprachen[881].pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Praxisbericht_3\\Bewertung\\Drucken.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Praxisbericht_3\\Bewertung\\Ablauf_und_Reflexion_der_Praxisphase_Teil_B.pdf\n",
      "C:\\Users\\marle\\Desktop\\Ablauf_und_Reflexion_der_Praxisphase_Teil_A.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Java\\Klausurrelevant.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Java\\Folien.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\IT-Sicherheit\\Vorlesungen\\2_Web_Anwendungen.pdf\n",
      "C:\\Uni\\Studium\\Web Engineering 2\\1.2-HTTP.pdf\n",
      "C:\\Users\\marle\\OneDrive\\Studium\\Formale Sprachen & Automaten\\KIT_Formale-Sprachen[881].pdf\n"
     ]
    }
   ],
   "source": [
    "resultSet = ind.retrieve(\"Python Stroetmann\")\n",
    "if resultSet:\n",
    "    for elem in resultSet:\n",
    "        if elem.split('.')[1] != 'dat':\n",
    "            print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probleme\n",
    "\n",
    "- er findet *.dat-Dateien\n",
    "- Kontext der Wörter nicht einbezogen\n",
    "- kein Stemming bisher\n",
    "- Queries mit einem Buchstaben finden viele Dokumente (vor allem Formeln etc.) -> durch Stemming gelöst, falls nicht gewollt\n",
    "\n",
    "- Index.build() braucht sehr lange -> parallelisieren?\n",
    "- Index muss jedes Mal neu aufgebaut werden -> via Pickle, JSON oder XML speichern und ziehen bei Start des Programms\n",
    "- RAM Auslastung ist durch das abspeichern der Dokumenteninhalte sehr hoch, beschleunigt aber auch die retrievve-Funktion\n",
    "\n",
    "# Verbesserungen\n",
    "\n",
    "- zwei Mengen bilden -> einmal Schnitt und einmal Vereinigung\\Schnitt -> Dokumente im Schnitt höher ranken\n",
    "- Kontext (2 Wörter links, 2 Wörter rechts neben Wort x) speichern ?\n",
    "- RAM Auslastung verringern, indem nur die Texte von den schon gesuchten Dokumente in dem Document-Objekt gespeichert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
