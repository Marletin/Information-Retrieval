{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100%; !important } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML, display\n",
    "display(HTML('<style>.container { width:100%; !important } </style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beispiel-Implementierung: Lokale Suchmaschine\n",
    "\n",
    "## Ziel der Beispiel-Implementierung\n",
    "Im Folgenden wird eine Beispiel-Implementierung der zuvor theoretisch diskutierten Inhalte\n",
    "vorgestellt. Dabei wird eine lokale Suchmaschine entwickelt, welche in der Lage ist,\n",
    "PDF-Dateien auf einem lokalen Computer-System zu parsen, in einen invertierten Index\n",
    "aufzunehmen, sowie Suchanfragen eines Benutzers sinnvoll zu beantworten. Zur Relevanz-\n",
    "Bestimmung der Dokumente wird das TF-IDF-Maß, welches bereits vorgestellt wurde,\n",
    "genutzt. Das Speichern des Index wird mit der von Python mitgelieferte Datenstruktur Dictionary,\n",
    "welche im Grunde eine Hashmap ist, umgesetzt. Weiter werden Bibliotheken eingesetzt,\n",
    "welche einige Vorarbeit leisten und damit den Code der Beispiel-Implementierung\n",
    "auf das Wesentliche beschränken. Die Anwendung soll die grundlegende Arbeitsweise eines\n",
    "Information Retrieval-Systems darlegen.\n",
    "\n",
    "## Genutzte Bibliotheken\n",
    "Vor der eigentlichen Implementierung der lokalen Suchmaschine werden einige Module eingebunden, welche die Implementierung unterstützen. Folgend werden die Module aufgelistet und wichtige Module im jeweiligen Abschnitt genauer erläutert:\n",
    "- Apache Tika: Erkennt und extrahiert Metadaten und Texte aus über tausend Dateitypen\n",
    "- Math: Standard Python-Modul für mathematische Funktionen\n",
    "- OS: Stellt Betriebssystem-Funktionalitäten bereit. Wird hier genutzt um durch Verzeichnisse zu navigieren\n",
    "- Python Magic: Stellt Funktionen zur Dateityp-Erkennung zur Verfügung\n",
    "- re: Das re-Modul stellt Funktionen bereit, mit denen mit regulären Ausdrücken gearbeitet werden kann. In der Beispiel-Implementierung werden mit diesem Modul die Tokens ermittelt\n",
    "- Platform: Prüfung, auf welchem Betriebssystem das Programm läuft, da Windows- und Linux-Systeme verschiedene Dateisysteme nutzen\n",
    "- Operator: Dieses Modul exportiert effiziente Funktionen, die den eigentlichen Operatoren von Python entsprechen. Es findet nur Anwendung in der Sortierung der zu zurückgebenden Dokumente in der retrieve-Methode.\n",
    "- NLTK: Stellt Funktionen zur Verfügung, die es ermöglichen mit menschlichen Sprachdaten zu arbeiten.\n",
    "\n",
    "### Apache Tika\n",
    "Bei Apache Tika handelt es sich um ein Framework um Inhalte aus Dateien zu erkennen und zu analysieren. Es ist in der Lage Text und Metadaten aus über tausend verschiedenen Arten von Dateien zu extrahieren. Tika liefert einen Parser, mit dessen Hilfe der Text aus - unter anderem - PDF-Dateien extrahiert werden kann. Mit dem Aufruf <i>parser.from_file(file)</i> kann eine PDF-Datei in reinen Text umgewandelt werden. Die Funktion liefert ein Dictionary zurück, welches einen Key content besitzt, über den auf den Inhalt der PDF-Datei zugegriffen werden kann.\n",
    "\n",
    "### python-magic\n",
    "Mittels python-magic ist es möglich, unabhängig von der Dateiendung, den Typ einer Datei zu ermitteln. Dies hat den Vorteil, dass die Suchmaschine sowohl unter Windows, als auch unter Unix-Systemen, alle PDF-Dateien finden kann, da unter Unix die Dateiendung keine garantierten Rückschlüsse auf den Typ der Datei zulässt.\n",
    "\n",
    "### NLTK\n",
    "NLTK (natural language toolkit) ist eine Bibliothek für Python, die für die Verarbeitung natürlicher Sprachen eingesetzt wird. Dabei bietet die Bibliotheken Funktionen für unter anderem Textklassifikation, Tokenization und Stemming. In diesem Beispiel wird nltk verwendet, um die Eingabetexte der Dokumente und die Eingaben des Nutzers zu Tokens zu verarbeiten. Dazu wird eine Klasse verwendet, die auf Basis von Regular Expressions arbeitet. Mehr dazu wird anhand der Implementierung gezeigt.\n",
    "\n",
    "### Platform\n",
    "Das Platform-Modul wird in dieser Implementierung dazu verwendet, festzustellen, auf welchem Betriebssystem die lokale Suchmaschine ausgeführt wird. Dies muss ermittelt werden, da auf Windows und Linux unterschiedliche Dateisysteme arbeiten. In Linux ist das Startverzeichnis immer das Root-Verzeichnis („/“). Unter Windows gibt meist mehrere Partitionen, die alle durchsucht werden müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'filetype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0387d2f0dd51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtika\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfiletype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'filetype'"
     ]
    }
   ],
   "source": [
    "from tika import parser\n",
    "import filetype\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "import platform\n",
    "import operator\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Document-Klasse\n",
    "Das Speichern der für das Retrieval wichtigen Informationen, geschieht mittels einer Document-Klasse. Diese Klasse hält alle Member-Variablen, die wichtig sind, um das TF-IDF-Maß berechnen zu können. Diese Member-Variablen sind:\n",
    "- url: String mit dem Pfad zum Dokument, welches von dieser Instanz repräsentiert wird\n",
    "- length: Integer, welcher die Anzahl der Wörter in diesem Dokument darstellt\n",
    "- docId: Integer, welcher dem Dokument einen eindeutigen Identifikator gibt\n",
    "- score: Float, welcher die Gewichtung der Dokument-Instanz zu einer Anfrage angibt\n",
    "- termList: Eine Liste von Strings, die die Terme des Dokumentes entsprechen. Diese wird durch die Methode _preprocess erzeugt\n",
    "\n",
    "Die Member-Variable <i>url</i> soll am Ende des Retrievalprozesses zurückgegeben werden, da der Nutzer durch den Pfad direkten Zugriff auf das Dokument bekommt. Die Member-Variablen <i>length</i>, <i>termList</i> und <i>score</i> werden für die Berechnung des TF-IDF-Maßes benötigt und so auch für den Retrievalprozess. Die Member-Variable <i>docId</i> wird in der Indexklasse einerseits genutzt, um die Identifier in dem <i>hashmap</i>-Dictionary den jeweiligen Termen zuzuordnen. Des Weiteren erfolgt die Zuordnung der jeweiligen ID zu ihrer Document-Instanz über die <i>docHashmap</i> in der Indexklasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, url, length, docId, termList):\n",
    "        self.url = url\n",
    "        self.length = length\n",
    "        self.id = docId\n",
    "        self.score = 0.\n",
    "        self.termList = termList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Die Document-Klasse hält neben den benötigten Attributen auch die Scoringmethode <i>tf_idf</i>. Dies ist die Implementierung des TF-IDF-Maßes und berechnet für jedes Dokument die Gewichtung für eine gegebene Suchquery aus. Hierbei benötigt die Funktion die Terme der Suchquery, welche über das Attribut terms übergeben werden. Das Attribut <i>df</i> steht für die Document Frequency, welches für jeden Term die Anzahl der gefunden Dokumente in Form eines Dictionaries beinhaltet. Über das Attribut <i>fileCount</i> wird die Anzahl der Dokumente in der Kollektion, hier die Dateien die im invertierten Index aufgenommen wurden, übergeben.\n",
    "\n",
    "Als erstes wird das Dictionary <i>tfDict</i> für die Term Frequency erstellt und mit den Termen der Query als Schlüssel und den Werten 0 initialisiert. In der nächsten For-Schleife wird über <i>self.termList</i> iteriert, welches die Liste der Terme des Dokumentes sind. Wenn ein Term auch in der Suchquery enthalten ist, also <i>if term in terms</i>, dann wird im <i>tfDict</i> der Wert des gefunden Terms um eins aufaddiert. Am Ende enthält das <i>tfDict</i> die Terme der Suchquery als Schlüssel und die Anzahl ihrer Vorkommnisse im Dokument als Wert, also die Term Frequency. Als letztes muss jede Term Frequency mit der Inverse Document Frequency multipliziert werden. Dafür wird über das Dictionary <i>df</i> iteriert und für jedes Key-Value-Paar die Inverse Document Frequency ausgerechnet und auf die jeweilige Term Frequency multipliziert. Am Ende wird die Summe aller TF-IDF-Maße der <i>score</i>-Variable zugeordnet und bilden so die finale Gewichtung für die gegebene Suche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(self, terms, df, fileCount):\n",
    "    \n",
    "    tfDict = {}\n",
    "    for term in terms:\n",
    "        tfDict[term] = 0\n",
    "    \n",
    "    ind = Index()\n",
    "        \n",
    "    for term in self.termList:\n",
    "        if term in terms:\n",
    "            tfDict[term] = tfDict[term]+1\n",
    "\n",
    "    for key, value in df.items():\n",
    "        idf = math.log((ind.fileCount/value+1),10)\n",
    "        tfDict[key] = tfDict[key]*idf\n",
    "    \n",
    "    self.score = sum(tfDict.values())\n",
    "\n",
    "Document.tf_idf = tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Der Index\n",
    "Nachdem die benötigten Bibliotheken bekannt sind, kann der Index implementiert werden. Bevor dieser jedoch aufgebaut werden kann, sind einige Vorarbeiten nötig, die durch die vorgestellten Bibliotheken gestützt werden.\n",
    "Der Index wird im Folgenden als Klasse implementiert. Diese beinhaltet die folgenden Methoden, die in den folgenden Abschnitten genauer diskutiert werden:\n",
    "- buildIndex()\n",
    "- retrieve()\n",
    "- calcTFIDF()\n",
    "\n",
    "Weiter werden die folgenden Member-Variablen benötigt:\n",
    "- hashmap\n",
    "- fileCount\n",
    "- docHashmap\n",
    "\n",
    "Die Member-Variable _hashmap_ ordnet allen Termen eine Menge von eindeutigen Dokumenten-IDs zu, in denen sie vorkommen. Im Dictionary _docHashmap_ werden die Dokuemnten-IDs als Key genutzt, um eine Zurodnung von Dokumenten-IDs auf Document-Objekte zu ermöglichen. Die Variable _fileCount_ ist ein Integer und wird für jedes gefundene Dokument um _1_ hochgezählt. Damit ist diese Variable qualifiziert als eindeutige Dokumenten-ID zu fungieren, wofür sie genutzt wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Index:\n",
    "    hashmap = {}\n",
    "    fileCount = 0\n",
    "    docHashmap = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### buildIndex\n",
    "Die Methode _buildIndex_ baut - wie der Name bereits vermuten lässt - den Index auf. Dabei dient ein Dictionary als Basis-Datenstruktur.\n",
    "\n",
    "Der erste Schritt stellt das Iterieren über alle Directories dar. Gestartet wird bei Linux-Systemen im Root-Directory, unter Windows-Systemen muss über jede Partition iteriert werden. Als nächstes wird über alle Dateien in den Verzeichnissen iteriert. Für jede Datei wird durch python-magic ermittelt, ob es sich um ein pdf-Dokument handelt. Ist ein Dokument vom Typ _pdf_, wird mithilfe von tika der Text aus dem pdf-Dokument extrahiert. \n",
    "\n",
    "Für jede entdeckte pdf-Datei wird ein Zähler erhöht, welcher eine eindeutige Dokumenten-ID darstellt. Anschlißend wird mittels der Hilfsmethode _\\__processText_ der Text der pdf-Dateien normalisiert. Diese Methode wird weiter unten genauer betrachtet.\n",
    "\n",
    "Die letzten Schritte beinhalten das Anlegen eines neuen Dokumenten-Objekts, welches in das Dictionary _docHashmap_ eingefügt wird. Zudem wird die Dokumenten-ID mithilfe der Hilfsmethode  _\\__addToIndex_ dem Dictionary _hasmap_ hinzugefügt, welches den eigentlichen Index enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildIndex(self, startDirectories=\"\"):\n",
    "    \n",
    "    if startDirectories == \"\":\n",
    "        startDirectories = self._getStartDirectories()\n",
    "        \n",
    "    for directory in startDirectories:\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                \n",
    "                path = os.path.abspath(os.path.join(root, file))\n",
    "                \n",
    "                try:\n",
    "                    if filetype.guess(path).mime == 'application/pdf':\n",
    "\n",
    "                        fileData = parser.from_file(path)\n",
    "                        rawText = fileData['content']\n",
    "                        self.fileCount += 1\n",
    "                    \n",
    "                        processedText = self._preprocessText(rawText)\n",
    "                        document = Document(path, len(processedText), self.fileCount, processedText)\n",
    "                        self.docHashmap.update({self.fileCount : document})\n",
    "                        self._addToIndex(self.fileCount, processedText)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "Index.buildIndex = buildIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsmethoden\n",
    "In diesem Abschnitt werden die genutzten Hilfsmethoden vorgestellt und am Code erklärt, wie die Funktionsweise implementiert wurde.\n",
    "\n",
    "#### _getStartDirectories\n",
    "Die Methode <i>\\_getSartDirectories</i> liefert eine Liste der Start-Verzeichnisse, abhängig vom Betriebssystem auf dem die Suchmaschine läuft. In diesen Verzeichnissen werden rekursiv nach PDF-Dateien gesucht, welche in den Index mit einfließen. Falls das zugrunde liegende Betriebssystem ein Linux-basiertes (Rückgabe Linux) oder Mac-basiertes (Rückgabe Darwin) System ist, wird die Liste <i>[\"/\"]</i> zurückgegeben, da das Verzeichnis <i>/</i> immer das root-Verzeichnis ist. Bei auf Windows basierenden Systemen gibt es wiederum mehrere Partitionen, welche immer mit einem Großbuchstaben abgekürzt, und somit auch mehrere root-Verzeichnisse.\n",
    "\n",
    "Zuerst wird geprüft welches Betriebssystem vorliegt. Bei einem auf Linux- oder Mac-basierenden System wird einfach eine List mit dem Element erstellt. Bei einem auf Windows basierenden Betriebssystem ist das erstellen der Startverzeichnisse aufwändiger. Hierbei werden alle Großbuchstaben darauf geprüft eine Partition zu sein. Dies wird mithilfe der <i>os.path.exists</i>-Methode realisiert. Ist ein Großbuchstabe tatsächliche eine Partition auf dem Computer, so wird er in der Liste gespeichert. Jedoch wird an den Großbuchstaben noch der String <i>\":\\\"</i> angehangen, damit die buildIndex-Methode mit den Elementen als Start-Verzeichnisse arbeiten kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _getStartDirectories(self):\n",
    "\n",
    "    if platform.system() == \"Linux\":\n",
    "        directories = [\"/\"]\n",
    "        \n",
    "    elif platform.system() == \"Darwin\":\n",
    "        directories = [\"/\"]\n",
    "        \n",
    "    elif platform.system() == \"Windows\":\n",
    "        directories = ['%s:\\\\' % d for d in string.ascii_uppercase if os.path.exists('%s:' % d)]\n",
    "        \n",
    "    else:\n",
    "        raise EnvironmentError\n",
    "        \n",
    "    return directories\n",
    "\n",
    "Index._getStartDirectories = _getStartDirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _addToIndex\n",
    "\n",
    "Die Methode <i>\\_addToIndex</i> soll die Dokumenten ID zu den invertierten Index der in dem Dokument vorkommenden Terme hinzufügen. Hierfür bekommt die Methode eine Liste von Termen die in einem PDF-Dokument vorkommen und die zum Dokumente gehörige Document ID als Parameter übergeben. Für jeden Term in der Liste <i>terms</i> wird dazu der zum Term gehörige Menge im invertierten Index  Schlägt der Versuch, die Menge für den Term term aus dem Dictionary zu holen, fehl, existiert noch keine Menge. In diesem Fall wird eine neue Menge erstellt und für den Term term ein Eintrag im Dictionary hinzugefügt, der auf die neu erstellte Menge referenziert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _addToIndex(self, documentID, terms):\n",
    "    \n",
    "    for term in terms:\n",
    "        \n",
    "        try:\n",
    "            docSet = self.hashmap[term]\n",
    "            docSet.add(documentID)\n",
    "            self.hashmap.update({term : docSet})\n",
    "            \n",
    "        except KeyError:\n",
    "            docSet = {documentID}\n",
    "            self.hashmap.update({term : docSet})\n",
    "    \n",
    "Index._addToIndex = _addToIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _preprocessText\n",
    "Diese Methode dient der Vorverarbeitung der Texte, die in den pdf-Dokumenten stehen. Hierfür wird der Text eines PDF-Dokumentes an den Parameter _text_ übergeben. Als erster Schritt wird der gesamte Text in Lower-Case (Kleinschreibung) gesetzt, damit später bei der Suche die Groß- bzw. Kleinschreibung irrelevant ist. Das Ergebnis wird in der Variable _lowerText_ gespeichert.\n",
    "Im nächsten Schritt werden alle Zahlen aus _lowerText_ entfernt und in _prepText_ gespeichert, da Zahlen für die Textsuche nicht von Bedeutung sind.\n",
    "\n",
    "Als nächstes wird mithilfe der Klasse _RegexpTokenizer_, die durch die nltk-Bibliothek zur Verfügung gestellt wird, der String _prepText_ in eine Liste von Tokens aufgespalten. Was als Token gewertet wird, wird mithilfe einer _regular expression_ definiert, im Deutschen regulärer Ausdruck genannt. Ein regulärer Ausdruck ist eine Zeichenkette, welche eine Menge von bestimmten Zeichenketten beschreibt. Der gewünschte reguläre Ausdruck wird dem Konstruktor der _RegexpTokenizer_-Klasse in Form eines raw Strings übergeben. Ein raw String ist ein String, welcher mit einem _r_ am Anfang gekennzeichnet ist und ein Backslash (\\\\) als ein Literal behandelt und nicht als ein Escape-Zeichen. Dies ist bei regulären Ausdrücken nützlich, da in diesen viel mit Backslashes gearbeitet wird.\n",
    "\n",
    "Im Folgenden wird der raw String bzw. reguläre Ausdruck näher betrachtet, um zu verstehen, was als ein Token gewertet wird. Der erste Teil des regulären Ausdrucks _[a-zA-Z]+-\\$_ definiert alle Buchstabenketten mit einen oder mehreren Elementen, die mit einem Bindestrich enden, als Token. Die eckigen Klammern werden bei regulären Ausdrücken genutzt, um eine Zeichenauswahl zu definieren. Das bedeutet, dass ein Zeichen aus dieser Auswahl dann an dieser Stelle steht. Mithilfe von Quantoren kann definiert werden, wie viele Zeichen einer Auswahl hintereinander stehen dürfen. Das Pluszeichen ist genau so ein Quantor, welcher aussagt, dass mindestens ein oder mehrere Zeichen der Zeichenauswahl hintereinander vorkommen muss.\n",
    "Das Dollarzeichen definiert das Ende einer Zeichenkette. Dadurch das ein Bindestrich vor das Dollarzeichen des regulären Ausdrucks gesetzt haben, bedeutet der reguläre Teilausdruck _-$_, dass die Zeichenkette auf einem Bindestrich endet. Bei dem |-Zeichen handelt es sich um eine logische Oderverknüpfung, die es ermöglicht, mehrere reguläre Ausdrücke zu verknüpfen. In unserem Fall _\\w+_.\n",
    "Der zweite reguläre Ausdruck _\\w+_ definiert alle alphanumerischen Zeichenketten mit einem oder mehreren Elementen als Token. Hierbei ist _\\w_ eine vordefinierte Zeichenklasse für den regulären Ausdruck <i>[a-zA-Z_0-9]</i> und beinhaltet außer alphanumerische Werte auch noch den Unterstrich. Das Pluszeichen ist hier wieder der Quantor, welcher aussagt, dass aus dieser Zeichenklasse ein oder mehrere Zeichen hintereinander vorkommen muss.\n",
    "Mithilfe der _tokenize_-Methode wird der reguläre Ausdruck auf den String _prepText_ angewendet. Jeder Substring des mitgegebenen Strings, der den regulären Ausdruck erfüllt, wird an die Liste _tokenList_ angefügt.\n",
    "\n",
    "Der Grund warum die Wörter, die auf einem Bindestrich enden, bei der Tokenerzeugen extra beachtet werden, ist der, dass die Wörter, welche bei Zeilenumbrüchen getrennt werden, wieder zusammengefügt werden sollen. In der for-Schleife werden diese Tokens auf die Eigenschaft hin, auf einem Bindestrich zu enden, geprüft und gegebenenfalls zusammengesetzt. Dazu wird der Bindestrich aus dem Token entfernt und mit dem nächsten Token in der Tokenliste verknüpft (_token[:-1]+tokenList[index+1]_). Die Tokenliste wird darauf hin aktualisiert. Der neu zusammengesetzte Token ersetzt den Token mit dem Bindestrich und der nächste Token der Liste wird gelöscht. \n",
    "\n",
    "Diese Methode ist jedoch nicht immer korrekt, denn es kann auch folgender Fall eintreten: Eine Zeile endet zum Beispiel mit _Damen-_ und die nächste Zeile geht mit _und Herrenschuhe_ weiter. In diesem Fall ist der Bindestrich gewollt, der Algorithmus fügt jedoch die Wörter _Damen_ und _und_ zu einem Wort zusammen. Da davon auszugehen ist, dass dieser Fall selten eintritt, wurde er aber vernachlässigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _preprocessText(self, text):\n",
    "    \n",
    "    lowerText = text.lower()\n",
    "    \n",
    "    prepText = re.sub(r'\\d+', '', lowerText)\n",
    "            \n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+-$|\\w+')\n",
    "    tokenList = tokenizer.tokenize(prepText)\n",
    "    \n",
    "    for token in tokenList:\n",
    "        if token[-1] == '-':\n",
    "            \n",
    "            index = tokenList.index(token)\n",
    "            compositeWord = token[:-1]+tokenList[index+1]\n",
    "            tokenList[index] = compositeWord\n",
    "            del tokenList[index+1]\n",
    "            \n",
    "    return tokenList\n",
    "    \n",
    "Index._preprocessText = _preprocessText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieve\n",
    "Die retrieve-Methode dient der Suche. Die Idee dabei ist, dass der Nutzer ein oder mehrere Schlagworte eingeben kann, auf deren Basis die am besten passenden Dokumente zurückgeliefert werden. Auch die Schlagworte werden den gleichen Normalisierungs-Prozess durchlaufen wie die Texte der Dokumente.\n",
    "\n",
    "Zunächst wird der Such-String des Nutzers mittels der bereits bekannten Methode _\\__preprocessText_ normalisiert. Im zweiten Schritt wird eine leere Menge angelegt, in der die Dokumenten-IDs, die zu den Termen gefunden werden, gespeichert. Mithilfe der for-Schleife wird über _processedStrings_ iteriert. Für jedes Wort wird versucht, die Menge aller Dokumenten-IDs zu dem Term _word_ aus dem Index zu beschaffen. Existiert die Menge zu dem Term _word_, wird die Vereinigung der bereits in der Menge _result_ stehenden Dokumenten-IDs und der mit dem Term _word_ gefundenen Dokumenten-IDs gebildet. Existiert der Term _word_ nicht als Key im Index, wird beim nächsten Term der Liste _processedStrings_ fortgefahren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve(self, searchString):\n",
    " \n",
    "    processedStrings = self._preprocessText(searchString)\n",
    "    result = set()\n",
    "    df = {}\n",
    "    helpDict = {}\n",
    "    resultList = []\n",
    "    \n",
    "    for word in processedStrings:\n",
    "        try:\n",
    "            documents = set(self.hashmap[word])\n",
    "            df[word] = len(documents)\n",
    "            result = result.union(documents)\n",
    "            \n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    for document in result:\n",
    "        doc = ind.docHashmap[document]\n",
    "        doc.tf_idf(processedStrings, df, self.fileCount)\n",
    "        helpDict[doc.socId] = doc.score\n",
    "        \n",
    "    sortedDict = sorted(helpDict.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    for key,_ in sortedDict:\n",
    "        resultList.append(ind.docHashmap[key].url)\n",
    "        \n",
    "    return resultList[::-1]\n",
    "\n",
    "Index.retrieve = retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = Index()\n",
    "ind.buildIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:\\Studium\\Java\\Folien.pdf\n",
      "M:\\Studium\\Java\\Klausurrelevant.pdf\n",
      "M:\\Studium\\Web Engineering 2\\restful-java-with-jax-rs-2-0-2rd-edition-en.pdf\n",
      "M:\\Studium\\Praxisbericht_3\\T3000_TINF16AIBI_Richter_Jesse-Jermaine.pdf\n",
      "M:\\Studium\\Verteilte Systeme\\Folien\\03-RMI.pdf\n",
      "M:\\Studium\\Compilerbau\\Java_Haskell_Compiler.pdf\n",
      "M:\\Studium\\Compilerbau\\Java_Haskell_Compiler3.pdf\n",
      "M:\\Studium\\Web Engineering 2\\5.1-JSP.pdf\n",
      "M:\\Studium\\Algorithmen\\algorithms.pdf\n",
      "M:\\Studium\\Formale Sprachen & Automaten\\KIT_Formale-Sprachen[881].pdf\n",
      "M:\\Studium\\Datenbanken\\10 Datenbanken I - Wiederholung, Vertiefung und Zusatzthemen.pdf\n",
      "M:\\Studium\\T2000\\Scan_JesseRichter_20180914124926.pdf\n",
      "M:\\Studium\\Compilerbau\\Java_Haskell_Compiler1.pdf\n",
      "M:\\Studium\\Compilerbau\\Java_Haskell_Compiler5.pdf\n",
      "M:\\Studium\\Java\\Klausur Java-Programmierung SS 2012.pdf\n",
      "M:\\Studium\\Praxisbericht_3\\Bewertung\\Drucken.pdf\n",
      "M:\\Studium\\Praxisbericht_3\\Bewertung\\Ablauf_und_Reflexion_der_Praxisphase_Teil_B.pdf\n",
      "M:\\Studium\\Praxisbericht_2\\Monitoring.pdf\n",
      "M:\\Studium\\Praxisbericht_2\\Monitoring-LAPTOP-2NADN4MG.pdf\n",
      "M:\\Studium\\Praxisbericht_2\\Monitoring-LAPTOP-2NADN4MG-2.pdf\n",
      "M:\\Studium\\Big Data\\Vorlesung\\BigDataAnalytics-Lecture-04.pdf\n",
      "M:\\Studium\\Web Engineering 2\\2.2-WBA-Architecture.pdf\n",
      "M:\\Studium\\Compilerbau\\folien-01.pdf\n",
      "M:\\Studium\\Compilerbau\\Java_Haskell_Compiler2.pdf\n",
      "M:\\Studium\\Verteilte Systeme\\Folien\\04-Namensdienste.pdf\n",
      "M:\\Studium\\Datenbanken II\\Datenbanken II.pdf\n",
      "M:\\Studium\\Compilerbau\\Java_Haskell_Compiler4.pdf\n",
      "M:\\Studium\\C\\Nachschreibklausur_Pr.pdf\n",
      "M:\\Studium\\Web Engineering\\20161208-Vorlesung-06.pdf\n",
      "M:\\Studium\\Verteilte Systeme\\Folien\\02-Middleware.pdf\n",
      "M:\\Studium\\Software Engineering 2\\2_DesignPatterns-MusterKlassifikation.pdf\n",
      "M:\\Studium\\C\\Klausur_J2.pdf\n",
      "M:\\Studium\\Web Engineering\\20170112-Vorlesung-08.pdf\n",
      "M:\\Studium\\E-Business\\E_COMMERCE_2017_1.pdf\n",
      "M:\\Studium\\Web Engineering 2\\1.2-servlets.pdf\n",
      "M:\\Studium\\C\\Klausur_J.pdf\n",
      "M:\\Studium\\Betriebssysteme\\betriebssysteme-01-17-09-21.pdf\n",
      "M:\\Studium\\Big Data\\Vorlesung\\BigDataAnalytics-Lecture-02.pdf\n",
      "M:\\Studium\\Web Engineering 2\\1.2-HTTP.pdf\n",
      "M:\\Studium\\Web Engineering 2\\3.1-Authentisierung_WebAppls.pdf\n",
      "M:\\Studium\\Verteilte Systeme\\Folien\\05-Prozessmanagement.pdf\n",
      "M:\\Studium\\Software Architecture Management\\SoftwareArchitektur_folien.pdf\n",
      "M:\\Studium\\IT-Sicherheit\\Vorlesungen\\2_Web_Anwendungen.pdf\n",
      "M:\\Studium\\IT-Sicherheit\\Zusammenfassung.pdf\n",
      "M:\\Studium\\Interaktive Systeme\\04 Interaktive Systeme - Hoeren.pdf\n",
      "M:\\Studium\\Compilerbau\\blatt2.pdf\n",
      "M:\\Studium\\Analysis\\analysis.pdf\n",
      "M:\\Studium\\Web Engineering 2\\5.4-ServerPush.pdf\n",
      "M:\\Studium\\Web Engineering 2\\5.2-Filterss.pdf\n",
      "M:\\Studium\\Web Engineering 2\\3.3-Servlets-API.pdf\n",
      "M:\\Studium\\Web Engineering\\20161027-Vorlesung-01.pdf\n",
      "M:\\Studium\\Studienarbeit\\irbookonlinereading.pdf\n",
      "M:\\Studium\\Logik\\logic.pdf\n",
      "M:\\Studium\\Kommunikations- Netzwerktechnik\\KT\\VKT00_Organisation_180213.pdf\n",
      "M:\\Studium\\Interaktive Systeme\\02 Interaktive Systeme - Sehen.pdf\n",
      "M:\\Studium\\Datenbanken\\Zusammenfassungen\\04 Relationale Modell und Algebra.pdf\n",
      "M:\\Studium\\Datenbanken\\Zusammenfassungen\\02 Geschichte und Motivation.pdf\n",
      "M:\\Studium\\Datenbanken\\05 Datenbanken I - Die Sprache SQL.pdf\n",
      "M:\\Studium\\Datenbanken\\02 Datenbanken I - Geschichte_Motivation_Grundlagen.pdf\n",
      "M:\\Studium\\Consulting\\Consulting.pdf\n",
      "M:\\Studium\\Compilerbau\\Compiler.pdf\n",
      "M:\\Studium\\Big Data\\Vorlesung\\BigDataAnalytics-Lecture-05.pdf\n",
      "M:\\Studium\\Big Data\\Vorlesung\\BigDataAnalytics-Lecture-00.pdf\n",
      "M:\\Studium\\Betriebssysteme\\betriebssysteme-02-17-09-28.pdf\n"
     ]
    }
   ],
   "source": [
    "resultSet = ind.retrieve(\"Jesse Richter Java\")\n",
    "if resultSet:\n",
    "    for elem in resultSet:\n",
    "        if elem.split('.')[-1] != 'dat':\n",
    "            print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:\\Studium\\Algorithmen\\algorithms.pdf\n",
      "M:\\Studium\\Analysis\\analysis.pdf\n",
      "M:\\Studium\\Statistik\\statistik.pdf\n",
      "M:\\Studium\\Lineare Algebra\\lineare-algebra.pdf\n",
      "M:\\Studium\\Logik\\logic.pdf\n",
      "M:\\Studium\\Studienarbeit\\irbookonlinereading.pdf\n",
      "M:\\Studium\\Wissenbasierte Systeme\\artificial-intelligence.pdf\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
