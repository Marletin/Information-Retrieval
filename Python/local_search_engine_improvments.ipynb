{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML, display\n",
    "display(HTML('<style>.container { width:100%; !important } </style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import magic\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "import platform\n",
    "import re\n",
    "import operator\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, url, length, id, textList):\n",
    "        self.url = url\n",
    "        self.length = length\n",
    "        self.id = id\n",
    "        self.score = 0.\n",
    "        self.textList = textList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Index:\n",
    "    hashmap = {} #dictionary\n",
    "    fileCount = 0 #integer, Gesamtzahl aller gefunden Dateien\n",
    "    docHashmap = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildIndex(self):\n",
    "    # alle Start-Verzeichnisse holen\n",
    "    start = self._getStartDirectories()\n",
    "    #start = [\"F:/Jonas/Uni\"]\n",
    "    #start = [\"C:/Users/marle/OneDrive/Studium\"]\n",
    "    # Magic-Instanz erstellen, um Datei-Typ bestimmen zu können\n",
    "    mime = magic.Magic(mime=True)\n",
    "    \n",
    "    for s in start:\n",
    "        for root, _dir, files in os.walk(s):\n",
    "            for f in files:\n",
    "                path = os.path.abspath(os.path.join(root, f))\n",
    "                try:\n",
    "                    if mime.from_file(path) == \"application/pdf\":\n",
    "                        # in Text umwawndeln und tokenization durchführen\n",
    "                        fileData = parser.from_file(path)\n",
    "                        rawText = fileData['content']\n",
    "                        self.fileCount += 1\n",
    "                    \n",
    "                        processedText = self._preprocessText(rawText)\n",
    "                        document = Document(path, len(processedText), self.fileCount, processedText)\n",
    "                        self.docHashmap.update({self.fileCount : document})\n",
    "                        self._addToIndex(self.fileCount, processedText)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "    return\n",
    "\n",
    "Index.buildIndex = buildIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _getStartDirectories(self):\n",
    "    start = []\n",
    "    \n",
    "    if platform.system() == \"Linux\":\n",
    "        start.append(\"/\")\n",
    "    elif platform.system() == \"Windows\":\n",
    "        start = ['%s:\\\\' % d for d in string.ascii_uppercase if os.path.exists('%s:' % d)]\n",
    "    else:\n",
    "        raise EnvironmentError\n",
    "        \n",
    "    return start\n",
    "\n",
    "Index._getStartDirectories = _getStartDirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _addToIndex(self, documentID, terms):\n",
    "    for t in terms:\n",
    "        try:\n",
    "            docs = self.hashmap[t]\n",
    "            docs.add(documentID)\n",
    "            self.hashmap.update({t : docs})\n",
    "        except KeyError:\n",
    "            docs = {documentID}\n",
    "            self.hashmap.update({t : docs})\n",
    "    \n",
    "Index._addToIndex = _addToIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _preprocessText(self, txt):\n",
    "    # lower all:\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    # remove digits\n",
    "    txt = re.sub(r'\\d+', '', txt)\n",
    "            \n",
    "    # tokenize the text\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+-$|\\w+')\n",
    "    result = tokenizer.tokenize(txt)\n",
    "    \n",
    "    # concatenate divided words\n",
    "    for word in result:\n",
    "        if word[-1] == '-':\n",
    "            ind = result.index(word)\n",
    "            corrected = word[:-1]+result[ind+1]\n",
    "            result[ind] = corrected\n",
    "            del result[ind+1]\n",
    "    return result\n",
    "    \n",
    "Index._preprocessText = _preprocessText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve(self, searchString):\n",
    "    # pre-processing\n",
    "    processedStrings = self._preprocessText(searchString)\n",
    "    result = set()\n",
    "    df = {}\n",
    "    helpDict = {}\n",
    "    resultList = []\n",
    "    \n",
    "    for word in processedStrings:\n",
    "        try:\n",
    "            documents = set(self.hashmap[word])\n",
    "            df[word] = len(documents)\n",
    "            result = result.union(documents)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    for document in result:\n",
    "        doc = ind.docHashmap[document]\n",
    "        doc.tf_idf(processedStrings,df)\n",
    "        helpDict[doc.id] = doc.score\n",
    "        \n",
    "    sortedDict = sorted(helpDict.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    for key,_ in sortedDict:\n",
    "        resultList.append(ind.docHashmap[key].url)\n",
    "        \n",
    "    return resultList[::-1]\n",
    "\n",
    "Index.retrieve = retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ind = Index()\n",
    "ind.buildIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(self, termList, df):\n",
    "    tfDict = {}\n",
    "    for term in termList:\n",
    "        tfDict[term] = 0\n",
    "    \n",
    "    ind = Index()\n",
    "        \n",
    "    for term in self.textList:\n",
    "        if term in termList:\n",
    "            tfDict[term] = tfDict[term]+1\n",
    "\n",
    "    for key, value in df.items():\n",
    "        idf = math.log((ind.fileCount+1/value+1),10)\n",
    "        tfDict[key] = tfDict[key]*idf\n",
    "    \n",
    "    self.score = sum(tfDict.values())\n",
    "\n",
    "Document.tf_idf = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultSet = ind.retrieve(\"Information Retrieval\")\n",
    "for elem in resultSet:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
