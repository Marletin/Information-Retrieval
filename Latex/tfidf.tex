Bei großen Sammlungen von Dokumenten reicht es nicht aus, wenn das IR die Dokumente zurückgibt, welche die Suchanfrage erfüllen. Die Menge der Dokumente, die durch das IR zurückgegebene werden, ist meist zu groß, so dass der Nutzer nicht in der Lage ist alle Dokumente zu sichten und die für ihn relevanten Dokument auszuwählen.\\
Die Frage, die sich hier stellt, ist, wie der Nutzer die Dokumente bekommt, die er mit hoher wahrscheinlichsten benötigt. Hier fällt der Begriff des Scorings. Scoring oder auch Bewertung wird genutzt um zu bestimmen, welche Dokumente für die Suchanfrage am relevantesten sind. Es kann auch von einer Gewichtung der Dokumente gesprochen werden.\\
Das Gewichtungsmodell, welches in dieser Arbeit thematisiert und genutzt wird, ist das TF-IDF-Maß. Da es sich bei der Eingabe des Nutzers um Freitext handelt und nicht um Terme, welche durch logische Operatoren verknüpft werden, bietet sich das TF-IDF-Maß an. TF steht hierbei für Term Frequency und IDF für Inverse Document Frequency. Beide Methoden werde einzeln in den folgenden Abschnitten vorgestellt und am Ende verknüpft.

\section{Term Frequency}
Um Dokumente aus einer Kollektion nach Termen einer Suchquery zu gewichten, ist das zählen der Terme $t$ einer Query in jedem Dokument $d$ die zunächst trivialste Lösung. Denn ein Dokument welches einen Term öfter besitzt, als ein anderes Dokument, hat mehr mit der Suchquery zu tun und muss dementsprechend höher gewichtet werden. Bei dieser Vorgehensweise wird von Term Frequency (siehe \cref{def:TF}) gesprochen, auf deutsch auch als Suchwortdichte bekannt. \newpage
\begin{defi}[Term Frequency]\label{def:TF}
	Die Term Frequency $tf_d,_t$ gibt die Anzahl des Terms $t$ in dem Dokument $d$ wider.
\end{defi}
Bei dieser Vorgehensweise werden jedoch nicht die Anordnung der Terme sowohl im Dokument, als auch in der Query berücksichtigt. Die Query \glqq Karl ist engagierter als Eckhard\grqq\ liefert die gleiche Gewichtung für die Dokumente einer Kollektion zurück wie \glqq Eckhard ist engagierter als Karl\grqq . Dieses Modell, das die Anordnung der Terme innerhalb des Dokumentes ignoriert, wird auch als \glqq Bag of Words Model\grqq\ bezeichnet und beschreibt, dass ein Dokument, eine Menge von Wörter und deren Anzahl ist. Das bedeutet, dass zwei Dokumente, welche die selben Wörter und die selbe Anzahl dieser haben, als identisch angesehen werden, obwohl sie das nicht unbedingt sind. Des Weiteren werden alle Terme als gleich wichtig gewertet, welches nicht immer zielführend ist, wie in Abschnitt \ref{sub:tokenerzeugung} mit den \glqq stop words\grqq\ beschrieben wurde.

\section{Inverse Document Frequency}
Da die Terme in einer Suchquery nicht alle von gleicher Bedeutung für die Suche sind, muss eine Lösung gefunden werden, wie die Terme untereinander gewichtet. Hier fällt der Begriff der Document Frequency (siehe \cref{def:DF})

\begin{defi}[Document Frequency]\label{def:DF}
	Die Document Frequency $df_t$ gibt die Anzahl der Dokumente $d$ in der genutzten Kollektion an, welche den Term $t$ besitzen.
\end{defi}

\begin{defi}[Inverse Document Frequency]\label{def:IDF}
	$IDF_t = log(\frac{N_D}{df_t})$
\end{defi}

Durch die Document Frequency kann festgestellt werden,

\section{Das TF-IDF-Maß}

\begin{defi}[TF-IDF]\label{defi:TF-IDF}
	Inhalt...
\end{defi}