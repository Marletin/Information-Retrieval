\section{Ziel der Beispiel-Implementierung}\label{ziel-der-beispiel-implementierung}

Im Folgenden wird eine Beispiel-Implementierung der zuvor theoretisch diskutierten Inhalte vorgestellt. Dabei wird eine lokale Suchmaschine entwickelt, welche in der Lage ist, PDF-Dateien auf einem lokalen Computer-System zu parsen, in einen invertierten Index aufzunehmen, sowie Suchanfragen eines Benutzers sinnvoll zu beantworten. Zur Relevanz-Bestimmung der Dokumente wird das TF-IDF-Maß, welches bereits vorgestellt wurde, genutzt. Das Speichern des Index wird mit der von Python mitgelieferte Datenstruktur \textit{Dictionary}, welche im Grunde eine Hashmap ist, umgesetzt. Weiter werden Bibliotheken eingesetzt, welche einige Vorarbeit leisten und damit den Code der Beispiel-Implementierung auf das Wesentliche beschränken. Die Anwendung soll die grundlegende Arbeitsweise eines Information Retrieval-Systems darlegen.

\section{Genutzte Bibliotheken}\label{genutzte-bibliotheken}

Vor der eigentlichen Implementierung der lokalen Suchmaschine werden einige Module eingebunden (siehe Abbildung \ref{fig:import}), welche die Implementierung unterstützen. Im Folgenden werden die Module aufgelistet und wichtige Module im jeweiligen Abschnitt genauer erläutert:\newpage
\begin{itemize}
	\item tika: Apache Tika erkennt und extrahiert Metadaten und Texte aus über tausend Dateitypen \cite{Apache_Tika}.
	\item math: Standard Python-Modul für mathematische Funktionen.
	\item os: Stellt Betriebssystem-Funktionalitäten bereit. Wird hier genutzt, um durch Verzeichnisse zu navigieren.
	\item filetype: Wird zur Dateityp-Erkennung genutzt \cite{filetype}.
	\item re: Das re-Modul stellt Funktionen bereit, mit denen mit regulären Ausdrücken gearbeitet werden kann. In der Beispiel-Implementierung werden mit diesem Modul die Tokens ermittelt \cite{re}.
	\item platform: Prüfung, auf welchem Betriebssystem das Programm läuft, da Windows- und Linux-Systeme verschiedene Dateisysteme nutzen.
	\item operator: Dieses Modul exportiert effiziente Funktionen, die den eigentlichen Operatoren von Python entsprechen. Es findet nur Anwendung in der Sortierung der zu zurückgebenden Dokumente in der retrieve-Methode.
	\item nltk: Stellt Funktionen zur Verfügung, die es ermöglichen mit menschlichen Sprachdaten zu arbeiten.
\end{itemize}

Die Module tika, filetype sowie nltk müssen per pip installiert werden und sind damit nicht standardmäßig in Python integriert.


\subsection{tika}\label{apache-tika}
Bei Apache Tika handelt es sich um eine Bibliothek, um Inhalte aus Dateien zu erkennen und zu analysieren. Es ist in der Lage Text und Metadaten aus verschiedenen Arten von Dateien zu extrahieren. Apache Tika liefert einen Parser, mit dessen Hilfe der Text aus - unter anderem - PDF-Dateien extrahiert werden kann. Mit dem Aufruf \textit{parser.from\_file(file)} kann ein PDF-Dokument in Text umgewandelt werden.

\subsection{filetype}\label{python-magic}
Mittels filetype ist es möglich, unabhängig von der Dateiendung, den Typ einer Datei zu ermitteln. Dies hat den Vorteil, dass die Suchmaschine sowohl unter Windows, als auch unter Unix-Systemen, alle PDF-Dateien finden kann, da unter Unix die Dateiendung keine garantierten Rückschlüsse auf den Typ der Datei zulässt.

\subsection{nltk}\label{nltk}
NLTK (natural language toolkit) ist eine Bibliothek für Python, die für die Verarbeitung natürlicher Sprachen eingesetzt wird. Dabei bietet die Bibliotheken Funktionen für unter anderem Textklassifikation, Tokenisierung und Stemming.
In diesem Beispiel wird NLTK verwendet, um die Eingabetexte der Dokumente und die Eingaben des Nutzers zu Tokens zu verarbeiten. Dazu wird eine Klasse verwendet, die auf Basis von regulären Ausdrücken arbeitet. Mehr dazu wird anhand der Implementierung gezeigt. %Zudem wird Stemming mithilfe von nltk durchgeführt, um Wörter auf ihren Wortstamm zurückzuführen. In den unteren Methoden wird näheres über die genutzten Operationen erläutert.

\subsection{platform}
Das Platform-Modul wird in dieser Implementierung dazu verwendet, festzustellen, auf welchem Betriebssystem die lokale Suchmaschine ausgeführt wird. Dies muss ermittelt werden, da auf Windows und Linux unterschiedliche Dateisysteme arbeiten. In Linux ist das Startverzeichnis immer das Root-Verzeichnis (\glqq /\grqq). Unter Windows gibt meist mehrere Partitionen, die alle durchsucht werden müssen.

\begin{figure}
	\rule{\textwidth}{0.4pt}
		\begin{lstlisting}[language=Python]
from tika import parser
import filetype
import math
import os
import string
import platform
import operator
import re
import pickle
from nltk.tokenize import RegexpTokenizer
		\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{Imports}
	\label{fig:import}
\end{figure}

\section{Die Document-Klasse}\label{die-document-klasse}

Das Speichern der für das Retrieval wichtigen Informationen geschieht mittels einer Document-Klasse (siehe Abbildung \ref{fig:document}). Diese Klasse hält unter anderem alle Member-Variablen, die wichtig sind, um das TF-IDF-Maß berechnen zu können. Die Member-Variablen der Klasse sind:
\begin{itemize}
	\item \textit{url}: String mit dem Pfad zum Dokument, welches von dieser Instanz repräsentiert wird.
	\item \textit{length}: Integer, welcher die Anzahl der Wörter in diesem Dokument darstellt.
	\item \textit{docId}: Integer, welcher dem Dokument einen eindeutigen Identifikator gibt.
	\item \textit{score}: Float, welcher die Gewichtung der Dokument-Instanz zu einer Anfrage angibt.
	\item \textit{termList}: Eine Liste von Strings, die den Termen des Dokumentes entsprechen. Diese wird durch die Methode \textit{\_preprocess} erzeugt (siehe Abschnitt \ref{preprocess}).
\end{itemize} 

\begin{figure}
	\rule{\textwidth}{0.4pt}
		\begin{lstlisting}[language=Python]
class Document:

  def __init__(self, url, length, docId, termList):
    self.url = url
    self.length = length
    self.docId = docId
    self.score = 0.
    self.termList = termList
		\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{Dokumentenklasse}
	\label{fig:document}
\end{figure}

Die Member-Variable \textit{url} soll am Ende des Retrievalprozesses zurückgegeben werden, da der Nutzer durch den Pfad direkten Zugriff auf das Dokument bekommt. Die Member-Variablen \textit{length}, \textit{termList} und \textit{score} werden für die Berechnung des TF-IDF-Maßes benötigt und so auch für den Retrievalprozess. Die Member-Variable \textit{docId} wird in der Indexklasse einerseits genutzt, um die Identifier in dem \textit{invIndex}-Dictionary den jeweiligen Termen zuzuordnen. Des Weiteren erfolgt die Zuordnung der jeweiligen ID zu ihrer Document-Instanz über die \textit{docHashmap} in der Indexklasse (siehe invIndex und docHashmap in Abschnitt \ref{der-index}).

\subsection{TF-IDF}\label{tf-idf}

Die Document-Klasse hält neben den benötigten Attributen auch die Scoringmethode \textit{tf\_idf} (siehe Abbildung \ref{fig:tfidf}). Dies ist die Implementierung des TF-IDF-Maßes und berechnet für jedes Dokument die Gewichtung für eine gegebene Suchanfrage. Hierbei benötigt die Funktion die Terme der Suchanfrage, welche über das Attribut \textit{queryTerms} übergeben werden. Das Attribut \textit{df} steht für die Document Frequency, welches für jeden Term der Anfrage die Anzahl der gefunden Dokumente in Form eines Dictionaries beinhaltet. Über das Attribut \textit{fileCount} wird die Anzahl der Dokumente in der Kollektion, hier die Dateien die im invertierten Index aufgenommen wurden, übergeben.

\begin{figure}
	\rule{\textwidth}{0.4pt}
		\begin{lstlisting}[language=Python]
def tf_idf(self, queryTerms, df, fileCount):

  tfDict = {}
  for term in queryTerms:
    tfDict[term] = 0  
  
  for term in self.termList:
    if term in queryTerms:
      tfDict[term] += 1

  for key, value in df.items():
    idf = math.log(fileCount/value+1)
    tfDict[key] *= idf

  self.score = sum(tfDict.values())

Document.tf_idf = tf_idf
		\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{TF-IDF-Methode}
	\label{fig:tfidf}
\end{figure}

Als erstes wird das Dictionary \textit{tfDict} für die Term Frequency erstellt und mit den Termen der Query als Schlüssel und den Werten 0 initialisiert. In der nächsten For-Schleife wird über \textit{self.termList} iteriert, welches die Liste der Terme des Dokumentes ist. Wenn ein Term auch in der Suchanfrage enthalten ist, also \textit{term in queryTerms} zu true evaluiert wird, wird im \textit{tfDict} der Wert des gefunden Terms um eins aufaddiert. Am Ende enthält das \textit{tfDict} die Terme der Suchanfrage als Schlüssel und die Anzahl ihrer Vorkommnisse im Dokument als Wert, also die Term Frequency. Als letztes muss jede Term Frequency mit der Inverse Document Frequency multipliziert werden. Dafür wird über das Dictionary \textit{df} iteriert und für jedes Key-Value-Paar die Inverse Document Frequency ausgerechnet und auf die jeweilige Term Frequency multipliziert. Am Ende wird die Summe aller TF-IDF-Maße der \textit{score}-Variable zugeordnet und bilden so die finale Gewichtung für die gegebene Suche.

\section{Der Index}\label{der-index}

Die Index-Klasse beinhaltet alle Methoden um den invertierten Index aufzubauen und die \textit{retrieve}-Methode, welche die Dokumente, sortiert nach deren Gewicht, zu einer gegebenen Query zurückgibt (siehe Abbildung \ref{fig:index}). Folgenden Member-Variablen werden für den invertierten Indexaufbau und die \textit{retrieve}-Methode benötigt:

\begin{itemize}
	\item \textit{invIndex}: Ist ein Dictionary mit einem Term als Schlüssel und einer Menge von Dokumenten-IDs als Wert.
	\item \textit{fileCount}: Zählt beim Aufbau des invertierten Indexes die Dokumente, die in diesem aufgenommen wurden.
	\item \textit{docHashmap}: Ist ein Dictionary welches eine Dokumenten-ID ihrer zugehörige Document-Klasseninstanz zuordnet.
\end{itemize}

Die Member-Variable \textit{invIndex} ist der invertierte Index welcher bei der \textit{retrieve}-Methode die Dokumenten-IDs zu einen gegebenen Term aus der Suchanfrage zurückgibt. Des Weiteren wird die Member-Variable \textit{docHashmap} dafür genutzt, um über die Dokumenten-IDs auf die jeweiligen Dokumentinstanzen zuzugreifen und so auf ihren Inhalt und die Scoringfunktion \textit{tf\_idf}. Die Member-Variable \textit{fileCount} gibt die Größe der Kollektion wieder und wird bei der Berechnung des TF-IDF-Maßes benötigt.

\begin{figure}
	\rule{\textwidth}{0.4pt}
		\begin{lstlisting}[language=Python]
class Index:
  
  def __init__(self):
    self.invIndex = {}
    self.fileCount = 0
    self.docHashmap = {}
		\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{Indexklasse}
	\label{fig:index}
\end{figure}

\subsection{buildIndex}\label{buildindex}

Die Methode \emph{buildIndex} baut den invertierten Index auf und nutzt dafür die\\
\textit{getStartDirectories}- und die \textit{\_addToIndex}-Methode. Dabei kann der \textit{buildIndex}-Methode optional Startverzeichnisse in Form einer Liste mit raw Strings mitgegeben werden. In der ersten If-Abfrage wird geprüft, ob die Liste \textit{startDirectories} leer ist. Dies ist der Fall, wenn der Nutzer keine Startverzeichnisse mitgegeben hat. Dann werden die Startverzeichnisse mithilfe der \textit{getStartDirectories}-Methode ermittelt.

Zuerst wird die Hilfsmetode \textit{cleanIndex}\ref{cleanIndex} aufgerufen. Diese löscht alle Dokumente aus dem Index, die nicht mehr auf dem System liegen.

Der zweite Schritt stellt das Iterieren über alle Startverzeichnisse der Liste \textit{startDirectories} dar. Anschließend werden für das Startverzeichnis, und alle darunterliegenden Verzeichnisse, bis zur untersten Ebene, die Dateien mithilfe der \textit{os.walk}-Funktion geholt. Für jede Datei wird dann mit den Funktionen \textit{os.path.abspath} und \textit{os.path.join} der absolute Pfad gebildet. Danach wird durch das Modul \textit{filetype} ermittelt, ob es sich um ein PDF-Dokument handelt. Handelt es sich um ein PDF-Dokument, wird im Dictionary \textit{dateMap} nachgeschlagen, ob das Dokument bereits bekannt ist. Wird ein Eintrag gefunden, erfolgt eine Prüfung, ob das Dokument den gleichen Zeitstempel besitzt wie er in \textit{dateMap} gespeichert ist. Falls sich das Datum unterscheidet, ist das Dokument nicht mehr aktuell und muss neu eingelesen werden. Tritt beim Nachschlagen in \textit{dateMap} ein KeyError auf, ist das Dokument nicht bekannt und muss ebenfalls eingelesen werden. Dass ein Dokument beim Aufbau des Index bereits hinterlegt ist, ist möglich, da der Index persistiert wird. 


 Die PDF-Dokumente werden mithilfe des \textit{Tika}-Moduls, genauer gesagt mit dem \textit{parser}-Objekt, der Text aus dem PDF-Dokument extrahiert. Dies geschieht, indem die Methode \textit{from\_file} des Parsers, mit dem Pfad der Datei als Argument, aufgerufen wird. Bei der Rückgabe kann mithilfe des Schlüssels \textit{content} auf den Text zugegriffen werden, welcher dann in der Variable \textit{rawText} gespeichert wird.

Für jede entdeckte PDF-Datei wird die Member-Variable \textit{fileCount} um eins erhöht. Da die Zahl sich bei jedem gefunden Dokument ändert, wird diese gleich als Dokumenten-ID genutzt, da sie für jedes Dokument eindeutig ist. Im Folgenden Schritt wird der \textit{rawText} mithilfe der \textit{preprocessText}-Methode normalisiert, in eine Liste von Tokens geteilt und in der Liste \textit{processedText} gespeichert. Nun sind alle Daten vorhanden um eine Documentinstanz zu erstellen. Diese enthält, wie in Abschnitt \ref{die-document-klasse} schon beschrieben, den Pfad zum Dokument, die Anzahl der Wörter, die Dokumenten-ID und den Text als Liste von Tokens.

Im vorletztem Schritt wird die erstellte Document-Instanz seiner Dokumenten-ID in der Member-Variable \textit{docHashmap} zugeordnet, um später auf diese Instanz zurückgreifen zu können. Als letztes wird der invertierte Index, mithilfe der \textit{\_addToIndex}-Methode, der Tokenliste \textit{processedText} und der Dokumenten-ID, aktualisiert.

\begin{figure}
	\rule{\textwidth}{0.4pt}
		\begin{lstlisting}[language=Python]
def buildIndex(self, startDirectories=[]):

  self._cleanIndex()
 
  if not startDirectories:
    startDirectories = getStartDirectories()

  for directory in startDirectories:
    for root, _, files in os.walk(directory):
      for file in files:

        path = os.path.abspath(os.path.join(root, file))

        try:
          if filetype.guess(path).mime == 'application/pdf':
            if self.dateMap[path][1] != os.path.getmtime(path):

              self.dateMap[path][1] = os.path.getmtime(path)
              docID = self.dateMap[path][0]
              fileData = parser.from_file(path)
              rawText = fileData['content']

              processedText = preprocessText(rawText)
              document = Document(path, len(processedText), docID,
                                  processedText)
              self.docHashmap[docID] = document
              self._addToIndex(docID, processedText)

       except KeyError:
         try:
           fileData = parser.from_file(path)
           rawText = fileData['content']
           self.fileCount += 1
           self.dateMap[path] = [self.fileCount, os.path.getmtime(path)]

           processedText = preprocessText(rawText)
           document = Document(path, len(processedText), self.fileCount,
                               processedText)
           self.docHashmap[self.fileCount] = document
           self._addToIndex(self.fileCount, processedText)
        except:
          continue

      except:
        continue

Index.buildIndex = buildIndex
		\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{BuildIndex-Methode}
	\label{fig:build}
\end{figure}

\subsection{Hilfsmethoden}\label{hilfsmethoden}

In diesem Abschnitt werden die drei Hilfsmethoden \textit{\_getStartDirectories}, \textit{\_preprocessText} und \textit{\_addToIndex} vorgestellt, welche in der \textit{buildIndex}-Methode genutzt werden.

\subsubsection{\_getStartDirectories}

Die Methode \textit{\_getSartDirectories} liefert eine Liste der Start-Verzeichnisse, abhängig vom Betriebssystem auf dem die Suchmaschine läuft. In diesen Verzeichnissen wird dann in der \textit{buildIndex}-Methode rekursiv nach PDF-Dateien gesucht, welche in den Index mit einfließen. Falls das zugrunde liegende Betriebssystem ein Linux-basiertes oder Mac-basiertes System ist, wird die Liste [\glqq /\grqq\ ] zurückgegeben, da das Verzeichnis / immer das Root-Verzeichnis ist. Bei einem auf Windows basierenden Systemen gibt es wiederum mehrere Partitionen, welche immer mit einem Großbuchstaben abgekürzt werden. Dementsprechend gibt es unter Windows auch mehrere Start-Verzeichnisse.

\begin{figure}
	\rule{\textwidth}{0.4pt}
	\begin{lstlisting}[language=Python]
def _getStartDirectories(self):
	
  if platform.system() == "Linux":
    directories = ["/"]
	
  elif platform.system() == "Darwin":
    directories = ["/"]
	
  elif platform.system() == "Windows":
    directories = ['%s:\\' % d for d in string.ascii_uppercase
                   if os.path.exists('%s:' % d)]
	
  else:
    raise EnvironmentError
	
  return directories
	
Index._getStartDirectories = _getStartDirectories
	\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{GetStartDirectories-Methode}
	\label{fig:start}
\end{figure}

Zuerst wird mithilfe der \textit{system}-Funktion des \textit{platform}-Moduls geprüft welches Betriebssystem vorliegt. Bei einem auf Linux- (Rückgabe \glqq Linux\grqq)oder Mac-basierenden System (Rückgabe \glqq Darwin\grqq) wird einfach eine List mit dem Element \glqq /\grqq\ erstellt.

Bei einem auf Windows basierenden Betriebssystem (Rückgabe \glqq Windows\grqq) ist das Erstellen der Startverzeichnisse aufwändiger. Hierbei werden alle Großbuchstaben, im Code durch \textit{string.ascii\_uppercase} aufrufbar, darauf geprüft eine Partition zu sein. Dies wird mithilfe der \textit{os.path.exists}-Methode realisiert. Ist ein Großbuchstabe tatsächliche eine Partition auf dem Computer, so wird er in der Liste gespeichert. Jedoch wird an den Großbuchstaben noch der String \glqq :$\backslash$\grqq\ angehangen, damit die \textit{buildIndex}-Methode mit den Elementen der Liste als Start-Verzeichnisse arbeiten kann.

\subsection{\_addToIndex}\label{addtoindex}

Die Methode \textit{\_addToIndex} fügt dem Index für jeden Term $t$ die Dokumenten-ID an, in welchem der Term $t$ gefunden wurde.  Hierfür bekommt die Methode eine Liste von Termen, die in einem PDF-Dokument vorkommen, über den Parameter \textit{terms} und die zum Dokument gehörige Dokumenten-ID als Parameter \textit{documentID} übergeben.

Für jeden Term in der Liste \emph{terms} wird dazu der zum Term gehörige Eintrag im invertierten Index nachgeschlagen. Schlägt der Versuch fehl, da noch kein Eintrag des Terms in dem invertierten Index vorhanden ist, wird ein neuer Eintrag für diesen Term und der übergebenen Dokumenten-ID erstellt. Wird ein Eintrag gefunden, wird die übergebende Dokumenten-ID der schon vorhandene Menge hinzugefügt und der invertierte Index wird aktualisiert.

\begin{figure}
	\rule{\textwidth}{0.4pt}
		\begin{lstlisting}[language=Python]
def _addToIndex(self, documentID, terms):
    
  for term in terms:
        
    try:
      docSet = self.invIndex[term]
      docSet.add(documentID)
      self.invIndex.update({term : docSet})
            
    except KeyError:
      self.invIndex[term] = {documentID}
    
Index._addToIndex = _addToIndex
		\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{AddToIndex-Methode}
	\label{fig:addToIndex}
\end{figure}


\subsection{\_cleanIndex}\label{cleanIndex}
Die Methode \textit{\_cleanIndex} dient dazu, alle PDF-Dokumente aus dem Index zu löschen, die nicht mehr auf dem System vorhanden sind. Dadurch kann es nicht mehr passieren, dass Dokumente in den Suchergebnissen erscheinen, die gar nicht mehr auf dem System existieren.

Der erste Schritt der in \ref{fig:cleanIndex} dargestellten Methode stellt das Iterieren über alle Einträge des Dictionarys \textit{dateMap} dar. Dadurch wird auf den Pfad sowie die eindeutige Dokumenten-ID für jedes Dokument zugegriffen.
Für jedes Dokument erfolgt eine Prüfung, ob das Dokument noch auf dem System vorhanden ist, dies geschieht mittels der \textit{isfile}-Methode des \textit{os}-Moduls. Falls das Dokument nicht mehr vorhanden ist, muss das Dokument aus \textit{docHasmap}, \textit{invIndex} sowie aus \textit{dateMap} gelöscht werden. 
Das Löschen erfolgt für \textit{docHashmap} in der ersten, für \textit{invIndex} in der zweiten und für \textit{dateMap} in der dritten for-Schleife.


\begin{figure}
	\rule{\textwidth}{0.4pt}
	\begin{lstlisting}[language=Python]
def _cleanIndex(self):

  docIDs = set()
  urls = []

  for url, values in self.dateMap.items():
    if not os.path.isfile(url):
      docID = values[0]
      urls.append(url)
      del self.docHashmap[docID]
      docIDs.add(docID)

  for term in self.invIndex:
    self.invIndex[term] -= docIDs

  for url in urls:
    del self.dateMap[url]

  print('Removed {} elements from Index'.format(len(docIDs)))

Index._cleanIndex = _cleanIndex

	\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{CleanIndex-Methode}
	\label{fig:cleanIndex}
\end{figure}

\subsection{retrieve}\label{retrieve}

Die \textit{retrieve}-Methode dient der Suche nach Dokumenten anhand einer eingegebenen Suchanfrage und stellt die Schnittstelle zum Nutzer dar. Die Idee dabei ist, dass der Nutzer ein oder mehrere Schlagworte, in Form eines Strings, der Methode übergeben kann. Auf deren Basis werden die gefundenen Dokumente, nach Relevanz sortiert, zurückgeliefert. 

Zunächst wird der Suchstring des Nutzers mittels der bereits bekannten Methode \textit{preprocessText} normalisiert und in eine Liste von Termen zerteilt. Das Ergebnis wird in der Variable \textit{processedStrings} abgespeichert. Im zweiten Schritt werden alle genutzten Variablen deklariert. Mithilfe der For-Schleife wird über die Liste von Termen \textit{processedStrings} iteriert, um über den invertierten Index für jeden Term die Menge der Dokumenten-IDs zu beschaffen. Des Weiteren wird für jeden Term die Länge seiner Menge von Dokumenten-IDs im Dictionary \textit{df} abgespeichert und repräsentiert so die Document Frequency. Die Variable \textit{result} stellt die Menge da, die alle zur Suchquery gefundenen Dokumenten-IDs enthält. So wird diese Variable mit jeder Menge von Dokumenten-IDs vereinigt. Da es sich hier um eine Menge handelt, kann ausgeschlossen werden, dass Dokumenten-IDs doppelt vorkommen. Existiert der Term \textit{word} nicht als Key im Index, da der Nutzer ein Wort eingegeben hat, welches kein Dokument beinhaltet, wird beim nächsten Term der Liste \emph{processedStrings} fortgefahren.

In der nächsten For-Schleife wird für jede Dokumenten-ID in der Liste \textit{result}, mithilfe der \textit{docHashmap}, die passende Documentinstanz geholt, um für jedes Dokument das TF-IDF-Maß auszurechnen. Wurde dies mithilfe der \textit{tf\_idf}-Methode getan, wird das Gewicht zu der jeweiligen Dokumenten-ID in dem Dictionary \textit{weightedDocs} gespeichert. Dieses wird mit der \textit{sorted}-Funktion von Python nach den Gewichten sortiert. Die Sortierung ist jedoch aufsteigend, obwohl es gewünscht  ist, die Dokumente mit einem höheren Gewicht oben zu haben. Dafür wird die Variable \textit{reverse} auf True gesetzt, damit die Einträge absteigend sortiert werden.

In der letzten For-Schleife werden die absoluten Pfade (\textit{ind.docHashmap[Key].url}), in der sortierten Reihenfolge, und die Gewichte der Dokumente für die Query in die Liste \textit{resultList} gespeichert. Diese wird am Ende zurückgegeben.

\begin{figure}
	\rule{\textwidth}{0.4pt}
	\begin{lstlisting}[language=Python]
def retrieve(self, searchString):
	
  processedStrings = preprocessText(searchString)
  result = set()
  df, weightedDocs = {}, {}
  resultList = []
	
  for word in processedStrings:
    try:
      documents = self.invIndex[word]
      df[word] = len(documents)
      result = result.union(documents)
    except KeyError:
      continue
	
  for document in result:
    doc = self.docHashmap[document]
    doc.tf_idf(processedStrings,df,self.fileCount)
    weightedDocs[doc.docId] = doc.score
	
  sortedDocs = sorted(weightedDocs.items(), key=operator.itemgetter(1),
                      reverse=True)
	
  for key, value in sortedDocs:
    resultList.append([ind.docHashmap[key].url, value])
	
  return resultList
	
Index.retrieve = retrieve
	\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{Retrieve-Methode}
	\label{fig:retrieve}
\end{figure}


\subsection{Hilfsmethoden}\label{hilfsmethoden}

In diesem Abschnitt werden die zwei Hilfsmethoden \textit{getStartDirectories} und \textit{preprocessText} vorgestellt, welche in der \textit{buildIndex}-Methode genutzt werden.

\subsubsection{getStartDirectories}

Die Methode \textit{getSartDirectories} liefert eine Liste der Start-Verzeichnisse, abhängig vom Betriebssystem auf dem die Suchmaschine läuft. In diesen Verzeichnissen wird dann in der \textit{buildIndex}-Methode rekursiv nach PDF-Dateien gesucht, welche in den Index mit einfließen. Falls das zugrunde liegende Betriebssystem ein Linux-basiertes oder Mac-basiertes System ist, wird die Liste [\glqq /\grqq\ ] zurückgegeben, da das Verzeichnis / immer das Root-Verzeichnis ist. Bei einem auf Windows basierenden Systemen gibt es wiederum mehrere Partitionen, welche immer mit einem Großbuchstaben abgekürzt werden. Dementsprechend gibt es unter Windows auch mehrere Start-Verzeichnisse.

\begin{figure}
	\rule{\textwidth}{0.4pt}
	\begin{lstlisting}[language=Python]
def getStartDirectories():
	
  if platform.system() == "Linux":
    directories = ["/"]
	
  elif platform.system() == "Darwin":
    directories = ["/"]
	
  elif platform.system() == "Windows":
    directories = ['%s:\\' % d for d in string.ascii_uppercase
                   if os.path.exists('%s:' % d)]
	
  else:
    raise EnvironmentError
	
  return directories
	\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{GetStartDirectories-Methode}
	\label{fig:start}
\end{figure}

Zuerst wird mithilfe der \textit{system}-Funktion des \textit{platform}-Moduls geprüft welches Betriebssystem vorliegt. Bei einem auf Linux- (Rückgabe \glqq Linux\grqq\ ) oder Mac-basierenden System (Rückgabe \glqq Darwin\grqq\ ) wird einfach eine List mit dem Element \glqq /\grqq\ erstellt.

Bei einem auf Windows basierenden Betriebssystem (Rückgabe \glqq Windows\grqq ) ist das Erstellen der Startverzeichnisse aufwändiger. Hierbei werden alle Großbuchstaben, im Code durch \textit{string.ascii\_uppercase} aufrufbar, darauf geprüft eine Partition zu sein. Dies wird mithilfe der \textit{os.path.exists}-Methode realisiert. Ist ein Großbuchstabe tatsächliche eine Partition auf dem Computer, so wird er in der Liste gespeichert. Jedoch wird an den Großbuchstaben noch der String \glqq :$\backslash$\grqq\ angehangen, damit die \textit{buildIndex}-Methode mit den Elementen der Liste als Start-Verzeichnisse arbeiten kann.

\subsubsection{preprocessText}\label{preprocess}
Diese Methode dient der Vorverarbeitung der Texte, die in den PDF-Dokumenten stehen, wird aber auch für die Suchanfrage genutzt. Hierfür wird der Text eines PDF-Dokumentes an den Parameter \textit{text} übergeben. Als erster Schritt wird der gesamte Text in Lower-Case (Kleinschreibung) gesetzt, damit später bei der Suche die Groß- bzw. Kleinschreibung irrelevant ist. Das Ergebnis wird in der Variable \textit{lowerText} gespeichert. Im nächsten Schritt werden alle Zahlen aus \textit{lowerText} entfernt, der Text ohne die Zahlen wird in \textit{prepText} gespeichert, da Zahlen für die Textsuche nicht von Bedeutung sind.

Als nächstes wird mithilfe der Klasse \textit{RegexpTokenizer}, die durch die NLTK-Bibliothek zur Verfügung gestellt wird, der String \textit{prepText} in eine Liste von Tokens aufgespalten. Was als Token gewertet wird, wird mithilfe einer \textit{regular expression} definiert, im Deutschen regulärer Ausdruck genannt. Ein regulärer Ausdruck ist eine Zeichenkette, welche eine Menge von bestimmten Zeichenketten beschreibt. Der gewünschte reguläre Ausdruck wird dem Konstruktor der \textit{RegexpTokenizer}-Klasse in Form eines raw Strings übergeben. Ein raw String ist ein String, welcher mit einem \textit{r} am Anfang gekennzeichnet ist und ein Backslash als ein Literal und kein Escape-Zeichen behandelt. Dies ist bei regulären Ausdrücken nützlich, da in diesen verstärkt mit Backslashs gearbeitet wird.

Im Folgenden wird der raw String bzw. reguläre Ausdruck näher betrachtet, um zu verstehen, was als ein Token gewertet wird. Der erste Teil des regulären Ausdrucks \textit{[a-zA-Z]+-\$} definiert alle Buchstabenketten mit einen oder mehreren Elementen, die mit einem Bindestrich bei einem Zeilenumbruch enden, als Token. Die eckigen Klammern werden bei regulären Ausdrücken genutzt, um eine Zeichenauswahl zu definieren. Das bedeutet, dass ein Zeichen aus dieser Auswahl dann an dieser Stelle steht. Mithilfe von Quantoren kann definiert werden, wie viele Zeichen einer Auswahl hintereinander stehen dürfen. Das Pluszeichen ist genau so ein Quantor, welcher aussagt, dass mindestens ein oder mehrere Zeichen der Zeichenauswahl hintereinander vorkommen muss. Die Funktion des Dollarzeichens hängt von einem Flag, dem Multiline-Flag ab. Dieses ist in NLTK standardmäßig gesetzt und bewirkt, dass das Dollarzeichen für das Ende einer Zeile steht.\cite{nltk} Sonst würde das Dollarzeichen das Ende eines Wortes markieren. Dadurch das ein Bindestrich vor das Dollarzeichen des regulären Ausdrucks gesetzt haben, bedeutet der reguläre Teilausdruck \textit{-\$}, dass die letzte Zeichenkette einer Zeile auf einem Bindestrich endet.

Bei dem \textbar{}-Zeichen handelt es sich um eine logische Oder-Verknüpfung, die es ermöglicht, mehrere reguläre Ausdrücke zu verknüpfen. In diesem Fall \textit{\textbackslash w+}. Der zweite reguläre Ausdruck \textit{\textbackslash w+} definiert alle alphanumerischen Zeichenketten mit einem oder mehreren Elementen als Token. Hierbei ist \textit{\textbackslash w} eine vordefinierte Zeichenklasse für den regulären Ausdruck \textit{[a-zA-Z\_0-9]} und beinhaltet außer alphanumerische Werte auch noch den Unterstrich. Das Pluszeichen ist hier wieder der Quantor, welcher aussagt, dass aus dieser Zeichenklasse ein oder mehrere Zeichen hintereinander vorkommen muss. Mithilfe der \textit{tokenize}-Methode wird der reguläre Ausdruck auf den String \textit{prepText} angewendet. Jeder Substring des mitgegebenen Strings, der den regulären Ausdruck erfüllt, wird an die Liste \textit{tokenList} angefügt.

Der Grund warum die Wörter, die auf einem Bindestrich enden, bei der Tokenerzeugen extra beachtet werden, ist der, dass die Wörter, welche bei Zeilenumbrüchen getrennt werden, wieder zusammengefügt werden sollen. In der for-Schleife werden diese Tokens auf die Eigenschaft hin, auf einem Bindestrich zu enden, geprüft und gegebenenfalls zusammengesetzt. Dazu wird der Bindestrich aus dem Token entfernt und mit dem nächsten Token in der Tokenliste verknüpft (\textit{token[:-1]+tokenList[index+1]}). Die Tokenliste wird darauf hin aktualisiert. Der neu zusammengesetzte Token ersetzt den Token, welcher den Bindestrich enthielt, und der darauffolgende Token der Liste wird gelöscht, da er schon mit dem Token davor zusammengesetzt wurde.

Diese Methode, die Wörter mit einem Bindestrich am Zeilenende mit dem nächsten Token zusammenzufügen, ist jedoch nicht immer korrekt. Es kann auch folgender Fall eintreten: Eine Zeile endet zum Beispiel mit \textit{Damen-} und die nächste Zeile geht mit \textit{und Herrenschuhe} weiter. In diesem Fall ist der Bindestrich gewollt, der Algorithmus fügt jedoch die Wörter \textit{Damen} und \textit{und} zu einem Wort zusammen. Da davon auszugehen ist, dass dieser Fall selten eintritt, wurde er vernachlässigt.

\begin{figure}
	\rule{\textwidth}{0.4pt}
		\begin{lstlisting}[language=Python]
def preprocessText(text):
    
  lowerText = text.lower()
    
  prepText = re.sub(r'\d+', '', lowerText)
            
  tokenizer = RegexpTokenizer(r'[a-zA-Z]+-$|\w+')
  tokenList = tokenizer.tokenize(prepText)
    
  for token in tokenList:
    if token[-1] == '-':
            
      index = tokenList.index(token)
      compositeWord = token[:-1]+tokenList[index+1]
      tokenList[index] = compositeWord
      del tokenList[index+1]
            
  return tokenList
		\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{PreprocessText-Methode}
	\label{fig:preprocess}
\end{figure}

\subsection{retrieve}\label{retrieve}

Die \textit{retrieve}-Methode dient der Suche nach Dokumenten anhand einer eingegebenen Suchanfrage und stellt die Schnittstelle zum Nutzer dar. Die Idee dabei ist, dass der Nutzer ein oder mehrere Schlagworte, in Form eines Strings, der Methode übergeben kann. Auf deren Basis werden die gefundenen Dokumente, nach Relevanz sortiert, zurückgeliefert. 

Zunächst wird der Suchstring des Nutzers mittels der bereits bekannten Methode \textit{preprocessText} normalisiert und in eine Liste von Termen zerteilt. Das Ergebnis wird in der Variable \textit{processedStrings} abgespeichert. Im zweiten Schritt werden alle genutzten Variablen deklariert. Mithilfe der For-Schleife wird über die Liste von Termen \textit{processedStrings} iteriert, um über den invertierten Index für jeden Term die Menge der Dokumenten-IDs zu beschaffen. Des Weiteren wird für jeden Term die Länge seiner Menge von Dokumenten-IDs im Dictionary \textit{df} abgespeichert und repräsentiert so die Document Frequency. Die Variable \textit{result} stellt die Menge da, die alle zur Suchanfrage gefundenen Dokumenten-IDs enthält. So wird diese Variable mit jeder Menge von Dokumenten-IDs vereinigt. Da es sich hier um eine Menge handelt, kann ausgeschlossen werden, dass Dokumenten-IDs doppelt vorkommen. Existiert der Term \textit{word} nicht als Key im Index, da der Nutzer ein Wort eingegeben hat, welches kein Dokument beinhaltet, wird beim nächsten Term der Liste \emph{processedStrings} fortgefahren.

In der nächsten For-Schleife wird für jede Dokumenten-ID in der Liste \textit{result}, mithilfe der \textit{docHashmap}, die passende Documentinstanz geholt, um für jedes Dokument das TF-IDF-Maß auszurechnen. Wurde dies mithilfe der \textit{tf\_idf}-Methode getan, wird das Gewicht zu der jeweiligen Dokumenten-ID in dem Dictionary \textit{weightedDocs} gespeichert. Dieses wird mit der \textit{sorted}-Funktion von Python nach den Gewichten sortiert. Die Sortierung ist jedoch aufsteigend, obwohl es gewünscht  ist, die Dokumente mit einem höheren Gewicht oben zu haben. Dafür wird die Variable \textit{reverse} auf True gesetzt, damit die Einträge absteigend sortiert werden.

In der letzten For-Schleife werden die absoluten Pfade (\textit{ind.docHashmap[Key].url}), in der sortierten Reihenfolge, und die Gewichte der Dokumente für die Query in die Liste \textit{resultList} gespeichert. Diese wird am Ende zurückgegeben.

\begin{figure}
	\rule{\textwidth}{0.4pt}
		\begin{lstlisting}[language=Python]
def retrieve(self, searchString):
 
  processedStrings = self._preprocessText(searchString)
  result = set()
  df, weightedDocs = {}, {}
  resultList = []
    
  for word in processedStrings:
    try:
      documents = self.invIndex[word]
      df[word] = len(documents)
      result = result.union(documents)
    except KeyError:
      continue
    
  for document in result:
    doc = ind.docHashmap[document]
    doc.tf_idf(processedStrings,df,self.fileCount)
    weightedDocs[doc.docId] = doc.score
        
  sortedDocs = sorted(weightedDocs.items(), key=operator.itemgetter(1),
                      reverse=True)
    
  for key, value in sortedDocs:
    resultList.append([ind.docHashmap[key].url, value])
        
  return resultList

Index.retrieve = retrieve
		\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{Retrieve-Methode}
	\label{fig:retrieve}
\end{figure}

\section{Ausführung der Suchmaschine}
In diesem Abschnitt soll kurz erläutert werden, wie die oben erstellten Klassen genutzt werden. Als erstes muss eine neue Instanz der Indexklasse erstellt werden. Über diese Instanz wird dann die \textit{buildIndex}-Methode aufgerufen. Optional kann hier eine Liste der gewünschten Startverzeichnisse als raw Strings übergeben werden. Wenn der invertierte Index aufgebaut wurde, kann über die Indexinstanz die \textit{retrieve}-Methode mit der gewünschten Suchanfrage aufgerufen werden. Diese liefert eine nach Gewichten sortierte Liste, mit den Pfaden zu den gefundenen Dokumenten. Wenn diese nicht leer ist, können mithilfe einer For-Schleife die Elemente ausgegeben werden.

\subsection{initIndex}\label{initIndex}
In der Hilfs-Funktion \textit{initIndex}, welche in \ref{fig:initIndex} dargestellt ist, wird der Index aufgebaut und mittels \textit{pickle} persistiert.

Zunächst wird geprüft, ob eine persistierte Version des Index gefunden werden kann, indem mittels der \textit{isfile}-Funktion des \textit{os}-Moduls geprüft wird, ob eine Index-Datei gefunden werden kann.
Ist dies der Fall, wird der Index mithilfe der \textit{load}-Funktion des \textit{pickle}-Moduls geladen. Diese Funktion nimmt als Argument eine Datei und liefert als Rückgabewert ein Python-Objekt, welches aus der mitgelieferten Datei gelesen wird.

\begin{figure}
	\rule{\textwidth}{0.4pt}
	\begin{lstlisting}[language=Python]
def initIndex(startDictionaries):

  if os.path.isfile('InvIndex'):
    with open('InvIndex','rb') as invIndex:
      index = pickle.load(invIndex)
  else:
    index = Index()

  index.buildIndex(startDictionaries)

  with open('InvIndex','wb') as invIndex:
    pickle.dump(index,invIndex)
  return index
	\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{Die InitIndex-Funktion}
	\label{fig:initIndex}
\end{figure}

Wird keine Index-Datei gefunden, wird eine neue Instanz der Index-Klasse erzeugt. Mithilfe der \textit{buildIndex}-Methode wird der Index aufgebaut bzw. aktualisiert, sofern dieser persistiert vorliegt.
\\
Zum Schluss wird der Index wieder persistiert. Dies wird von der \textit{dump}-Funktion von \textit{pickle} übernommen. Diese Funktion nimmt ein Python-Objekt sowie eine Datei als Argumente und schreibt das Objekt in diese Datei.

\begin{figure}
	\rule{\textwidth}{0.4pt}
		\begin{lstlisting}[language=Python]
ind = initIndex([path])
resultSet = ind.retrieve(query)
  if resultSet:
    print('Pfad, \t Gewicht')
    for url, score in resultSet:
      print(url, '\t', score)
		\end{lstlisting}
	\rule{\textwidth}{0.4pt}
	\caption{Execute}
	\label{fig:execute}
\end{figure}